{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccbeb3d88eb4663be1165e0345e94a87",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 29, 2023 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Sequence-to-sequence modeling with recurrent neural networks\n",
    "\n",
    "The goals of this exercise are\n",
    "* to get familiar with recurrent neural networks used for sequential data processing\n",
    "* to get familiar with the sequence-to-sequence model for machine translation\n",
    "* to learn PyTorch tools for batch processing of sequences with varying lengths\n",
    "* to learn how to write a custom `DataLoader`\n",
    "\n",
    "You may find it useful to look at this tutorial:\n",
    "* [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cc4d569dc32e40fe066146a07b7c7b7",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88cb529e1e7d2069f68d3df82a852cce",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset that we are going to use consists of pairs of sentences in French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa7408c0ea947cac397f5e35ce6498ee",
     "grade": false,
     "grade_id": "cell-6638bcc556bb02f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from data import TranslationDataset, MAX_LENGTH, SOS_token, EOS_token\n",
    "\n",
    "trainset = TranslationDataset(data_dir, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac3d437d5ade200a6ff8ca5407bf2c01",
     "grade": false,
     "grade_id": "cell-2d3c1dd7e239d2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* `TranslationDataset` supports indexing as required by `torch.utils.data.Dataset`.\n",
    "* Sentences are tensors of maximum length `MAX_LENGTH`.\n",
    "* Words in a (sentence) tensor are represented as an index (integer) in a language vocabulary.\n",
    "* The string representation of a word from the source language can be obtained from index `i` with `dataset.input_lang.index2word[i]`.\n",
    "* Similarly for the target language `dataset.output_lang.index2word[j]`.\n",
    "\n",
    "Let us look at samples from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b55856502471f9f6a7f4a7f3699504b5",
     "grade": false,
     "grade_id": "cell-0d793aaf021670ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: \"il se plaint tout le temps . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([  24,  533, 3048,   35,  454, 1423,    5,    1])\n",
      "Target sentence: \"he is always complaining . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([  14,   40,  778, 1246,    4,    1])\n"
     ]
    }
   ],
   "source": [
    "src_sentence, tgt_sentence = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence: \"%s\"' % ' '.join(trainset.input_lang.index2word[i.item()] for i in src_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(src_sentence)\n",
    "\n",
    "print('Target sentence: \"%s\"' % ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(tgt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d8116da735b2c5503a947814fd393b0",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source-target pairs in the training set:  8682\n"
     ]
    }
   ],
   "source": [
    "print('Number of source-target pairs in the training set: ', len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c5eec5bf6f7b88396659a1cdc5faa2c",
     "grade": false,
     "grade_id": "cell-80e91375dcb62ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence-to-sequence model for machine translation\n",
    "\n",
    "In this exercise, we are going to build a machine translation system which transforms a sentence in one language into a sentence in another one. The computational graph of the translation model is shown below:\n",
    "\n",
    "<img src=\"seq2seq.png\" width=900>\n",
    "\n",
    "We are going to use a simplified model without the dotted connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a752f56ff2754ca6c8d2165547a9f60",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "We would like to train the sequence-to-sequence model using mini-batch training.\n",
    "One difficulty of mini-batch training in this case is that sequences may have varying lengths and this has to be taken into account when building the computational graph. Luckily, PyTorch has tools to support batch processing of such sequences.\n",
    "To use those tools, we need to write a custom data loader which puts sequences of varying lengths in the same tensor. We can customize the data loader by providing a custom `collate_fn` as explained [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Our collate function:\n",
    "- combines sequences from the source language in a single tensor with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "- combines sequences from the target language in a single tensor with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "\n",
    "**Important**:\n",
    "- Later in the code (not in this `collate` function), we will convert source sequences to objects of class [PackedSequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence) which can be processed by recurrent units such as `GRU` or `LSTM`. Conversion to `PackedSequence` objects requires sequences to be sorted by their lengths.\n",
    "**Therefore, the returned source sequences should be sorted by length in a decreasing order.**\n",
    "* The target sequences need not be sorted by their lengths because we have to keep the same order of sequences in the source and target tensors.\n",
    "\n",
    "Your task is to implement the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "523f0d6ef1a7a3716d4f0e47fe2ada46",
     "grade": false,
     "grade_id": "cell-f7cf358c436c6d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d791959e8c9e2b324dec5a664ab99b26",
     "grade": false,
     "grade_id": "collate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length,)\n",
    "          tgt_seq is of shape (tgt_seq_length,)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "          The sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "          the longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "      src_seq_lengths: List of lengths of source sequences.\n",
    "      tgt_seqs of shape (max_tgt_seq_length, batch_size): Tensor of padded target sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "        \n",
    "    # The returned source sequences should be sorted by length in a decreasing order\n",
    "    # The target sequences need not be sorted by their lengths because we have to keep the same \n",
    "    # order of sequences in the source and target tensors.\n",
    "    \n",
    "    sorted_by_src_list = sorted(list_of_samples, key=lambda sequence: sequence[0].size(), reverse=True)\n",
    "    \n",
    "    # The separated source and target sequences\n",
    "    src_seqs = [sample[0] for sample in sorted_by_src_list]\n",
    "    tgt_seqs = [sample[1] for sample in sorted_by_src_list]\n",
    "    \n",
    "    # List of lengths of source sequences\n",
    "    src_seq_lengths = [len(src) for src in src_seqs]\n",
    "    \n",
    "    # Our collate function:\n",
    "    # - combines sequences from the source language in a single tensor with extra values \n",
    "    # (at the end) filled with `PADDING_VALUE=0`.\n",
    "    # - combines sequences from the target language in a single tensor with extra values \n",
    "    # (at the end) filled with `PADDING_VALUE=0`.\n",
    "    src_seqs = pad_sequence(src_seqs)\n",
    "    tgt_seqs = pad_sequence(tgt_seqs)\n",
    "    \n",
    "    return src_seqs, src_seq_lengths, tgt_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4556bc267765daec3cd760ab352b33a",
     "grade": false,
     "grade_id": "cell-946d5fa69247a122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), torch.LongTensor([3, 4, 5])),\n",
    "        (torch.LongTensor([6, 7, 8]), torch.LongTensor([9, 10])),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3, 2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "860e8fce2111d78161a058e93f9c1cff",
     "grade": true,
     "grade_id": "test_collate_fn",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences combined:\n",
      "tensor([[11,  6,  1],\n",
      "        [12,  7,  2],\n",
      "        [13,  8,  0],\n",
      "        [14,  0,  0]])\n",
      "[4, 3, 2]\n",
      "Target sequences combined:\n",
      "tensor([[15,  9,  3],\n",
      "        [ 0, 10,  4],\n",
      "        [ 0,  0,  5]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate() function\n",
    "\n",
    "def test_collate_fn():\n",
    "    pairs = [\n",
    "        (torch.tensor([1, 2]), torch.tensor([3, 4, 5])),\n",
    "        (torch.tensor([6, 7, 8]), torch.tensor([9, 10])),\n",
    "        (torch.tensor([11, 12, 13, 14]), torch.tensor([15])),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert pad_src_seqs.shape == torch.Size([4, 3]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3, 3]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    print('Source sequences combined:')\n",
    "    print(pad_src_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [11, 6, 1],\n",
    "      [12, 7, 2],\n",
    "      [13, 8, 0],\n",
    "      [14, 0, 0],\n",
    "    ])\n",
    "    assert (pad_src_seqs == expected).all(), \"pad_src_seqs does not match expected values\"\n",
    "\n",
    "    print(src_seq_lengths)\n",
    "    if isinstance(src_seq_lengths[0], torch.Size):\n",
    "        src_seq_lengths = sum((list(l) for l in src_seq_lengths), [])\n",
    "    else:\n",
    "        src_seq_lengths = [int(l) for l in src_seq_lengths]\n",
    "    assert src_seq_lengths == [4, 3, 2], f\"Bad src_seq_lengths: {src_seq_lengths}\"\n",
    "\n",
    "    print('Target sequences combined:')\n",
    "    print(pad_tgt_seqs)\n",
    "    expected = torch.tensor([\n",
    "      [15,  9, 3],\n",
    "      [ 0, 10, 4],\n",
    "      [ 0,  0, 5],\n",
    "    ])\n",
    "    assert (pad_tgt_seqs == expected).all(), \"pad_tgt_seqs0 does not match expected values\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc67bc5d1c00931a6cb4a0fdf840b71",
     "grade": false,
     "grade_id": "cell-b4fb631ef92b42cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3be8620d95e2c550cfb251537619dc9c",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder encodes a source sequence $(x_1, x_2, ..., x_T)$ into a single vector $h_T$ using the following recursion:\n",
    "$$\n",
    "  h_{t} = f(h_{t-1}, x_t) \\qquad t = 1, \\ldots, T\n",
    "$$\n",
    "where:\n",
    "* intial state $h_0$ is often chosen arbitrarily (we choose it to be zero)\n",
    "* function $f$ is defined by the type of the RNN cell (in our experiments, we will use [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU))\n",
    "* $x_t$ is a vector that represents the $t$-th word in the source sentence.\n",
    "\n",
    "A common practice in natural language processing is to _learn_ the word representations $x_t$ (instead of, for example, using one-hot coded vectors). In PyTorch, this is supported by class [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) which we are going to use.\n",
    "\n",
    "The computational graph of the encoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_encoder.png\" width=500>\n",
    "\n",
    "Your task is to implement the `forward` function of the encoder. It should contain the following steps:\n",
    "* Embed the words of the source sequences.\n",
    "* Pack source sequences using [`pack_padded_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence). This converts padded source sequences into an object that can be processed by PyTorch recurrent units such as `nn.GRU` or `nn.LSTM`.\n",
    "* Apply GRU computations to packed sequences obtained in the previous step\n",
    "* Convert packed sequence of GRU outputs into padded representation with [`pad_packed_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad27026a59c87defe4742f2afbcabd4",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_dictionary_size: The number of words in the source dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state of GRU.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "          seq_lengths: List of sequence lengths.\n",
    "          hidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "          hidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "  \n",
    "        # Apply GRU computations to packed sequences obtained in the previous step\n",
    "        # Convert packed sequence of GRU outputs into padded representation with [`pad_packed_sequence`]\n",
    "        \n",
    "        # Embed the words of the source sequences.\n",
    "        y = self.embedding(pad_seqs)\n",
    "        \n",
    "        # Pack source sequences using [`pack_padded_sequence`]. \n",
    "        # This converts padded source sequences into an object that can be processed by PyTorch \n",
    "        # recurrent units such as `nn.GRU` or `nn.LSTM`.\n",
    "        \n",
    "        # rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)\n",
    "        y = pack_padded_sequence(y, seq_lengths)\n",
    "        \n",
    "        # gru(input, h_0)\n",
    "        # The input can also be a packed variable length sequence\n",
    "        # h_0 contains the initial hidden state for the input sequence. \n",
    "        # Defaults to zeros if not provided\n",
    "    \n",
    "        # If a rnn.PackedSequence has been given as the input, \n",
    "        # the output will also be a packed sequence.\n",
    "        # hidden containing the final hidden state for the input sequence\n",
    "        outputs, hidden = self.gru(y, hidden)\n",
    "        \n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence()\n",
    "        # Returns:\n",
    "        # Tuple of Tensor containing the padded sequence, and a Tensor \n",
    "        # containing the list of lengths of each sequence in the batch\n",
    "        outputs, _ = pad_packed_sequence(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b26ce62da6c7f9b9f9370cac4ebe9d6",
     "grade": false,
     "grade_id": "cell-33422e03c9970c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Encoder_shapes():\n",
    "    hidden_size = 3\n",
    "    encoder = Encoder(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = encoder.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs, new_hidden = encoder.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, hidden_size]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape: {new_hidden.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44a4c17bee0657f2c7329fd181aa0e1d",
     "grade": true,
     "grade_id": "test_Encoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0221],\n",
      "        [ 0.0007, -0.0055],\n",
      "        [ 0.0005,  0.0323]])\n",
      "expected:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0221],\n",
      "        [ 0.0007, -0.0055],\n",
      "        [ 0.0005,  0.0323]])\n",
      "outputs[:2, 1, :]:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0021]])\n",
      "expected:\n",
      " tensor([[ 0.0000, -0.0150],\n",
      "        [ 0.0004, -0.0021]])\n",
      "new_hidden:\n",
      " tensor([[[ 0.0005,  0.0323],\n",
      "         [ 0.0004, -0.0021]]])\n",
      "expected:\n",
      " tensor([[[ 0.0005,  0.0323],\n",
      "         [ 0.0004, -0.0021]]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests Encoder\n",
    "import tests\n",
    "\n",
    "def test_Encoder(Encoder):\n",
    "    with torch.no_grad():\n",
    "        net = Encoder(src_dictionary_size=5, embed_size=2, hidden_size=2)\n",
    "        tests.set_weights_encoder(net)\n",
    "\n",
    "        pad_seqs = torch.tensor([\n",
    "            [1, 2],\n",
    "            [2, 3],\n",
    "            [3, 0],\n",
    "            [4, 0]\n",
    "        ])  # (max_seq_length, batch_size)\n",
    "        seq_lengths = [4, 2]\n",
    "        hidden = net.init_hidden(batch_size=2)\n",
    "\n",
    "        outputs, new_hidden = net.forward(pad_seqs, seq_lengths, hidden)\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [ 0.0000, -0.0150],\n",
    "            [ 0.0004, -0.0221],\n",
    "            [ 0.0007, -0.0055],\n",
    "            [ 0.0005,  0.0323]\n",
    "        ])\n",
    "        print('outputs[:, 0, :]:\\n', outputs[:, 0, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,0,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [ 0.0000, -0.0150],\n",
    "            [ 0.0004, -0.0021]\n",
    "        ])\n",
    "        print('outputs[:2, 1, :]:\\n', outputs[:2, 1, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:2,1,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        expected = torch.tensor([[\n",
    "            [ 0.0005,  0.0323],\n",
    "            [ 0.0004, -0.0021]\n",
    "        ]])\n",
    "        print('new_hidden:\\n', new_hidden)\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(new_hidden, expected, atol=1e-4), \"new_hidden does not match expected value\"\n",
    "        print('Success')\n",
    "\n",
    "test_Encoder(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "078cb4df64e3607355197dff52f07d9a",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder takes as input the representation computed by the encoder and transforms it into a sentence in the target language. The computational graph of the decoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_decoder.png\" width=500 align=\"top\">\n",
    "\n",
    "* $z_0$ is the output of the encoder, that is $z_0 = h_5$, thus `hidden_size` of the decoder should be the same as `hidden_size` of the encoder.\n",
    "* $y_{i}$ are the log-probabilities of the words in the target language, the dimensionality of $y_{i}$ is the size of the target dictionary.\n",
    "* $z_{i}$ is mapped to $y_{i}$ using a linear layer `self.out` followed by `F.log_softmax` (because we use `nn.NLLLoss` loss for training).\n",
    "* Each cell of the decoder is a GRU, it receives as inputs the previous state $z_{i-1}$ and relu of the **embedding** of the previous word. Thus, you need to embed the words of the target language as well. The previous word is taken as the word with the maximum log-probability.\n",
    "\n",
    "Note that the decoder outputs a word at every step and the same word is used as the input to the recurrent unit at the next step. At the beginning of decoding, the previous word input is fed with a special word SOS which stands for \"start of a sentence\". During training, we know the target sentence for decoding, therefore we can feed the correct words $y_i$ as inputs to the recurrent unit.\n",
    "\n",
    "There is one extra thing that it is wise to take care of. When the target sentence is fed to the decoder during training, the decoder learns to generate only the next word (this scenario is called \"teacher forcing\"). At test time, the decoder works differently: it generates the whole sequence using its own predictions as inputs at each step. Therefore, it makes sense to train the decoder to produce full sentences. In order to do that, we will alternate between two modes during training:\n",
    "* \"teacher forcing\": the decoder is fed with the words in the target sequence\n",
    "* no \"teacher forcing\": the decoder generates the output sequence using its own predictions. In this case, we will generate sequences of the same length as the length of the longest sequence in `pad_tgt_seqs` (if `pad_tgt_seqs` is not `None`) or of length `MAX_LENGTH` (if `pad_tgt_seqs` is `None`).\n",
    "\n",
    "You need to implement the decoder which has the structure shown in the figure above.\n",
    "\n",
    "Notes:\n",
    "* `SOS_token` is imported at the beginning of the notebook.\n",
    "* **Running this code on GPU sometimes fails producing a CUDA error (if you know the reason, please let us know).** If this happens to you, please train the model on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70b2c4f47ee96519de22564de0a2f192",
     "grade": false,
     "grade_id": "Decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_dictionary_size: The number of words in the target dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(tgt_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, tgt_dictionary_size)\n",
    "\n",
    "    def forward(self, hidden, pad_tgt_seqs=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden of shape (1, batch_size, hidden_size): States of the GRU.\n",
    "          pad_tgt_seqs of shape (max_out_seq_length, batch_size): Tensor of words (word indices) of the\n",
    "              target sentence. If None, the output sequence is generated by feeding the decoder's outputs\n",
    "              (teacher_forcing has to be False).\n",
    "          teacher_forcing (bool): Whether to use teacher forcing or not.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_out_seq_length, batch_size, tgt_dictionary_size): Tensor of log-probabilities\n",
    "              of words in the target language.\n",
    "          hidden of shape (1, batch_size, hidden_size): New states of the GRU.\n",
    "\n",
    "        Note: Do not forget to transfer tensors that you may want to create in this function to the device\n",
    "        specified by `hidden.device`.\n",
    "        \"\"\"\n",
    "        if pad_tgt_seqs is None:\n",
    "            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'\n",
    "        #print(\"Hidden shape\")\n",
    "        #print(hidden.shape)\n",
    "        batch_size = hidden.shape[1]\n",
    "        hidden_size = hidden.shape[2]\n",
    "        \n",
    "        SOS = SOS_token * np.ones(batch_size).reshape(1,-1)\n",
    "        SOS = torch.LongTensor(SOS) # SOS is of shape (1, batch_size)\n",
    "        \n",
    "        # In teacher forcing mode, the decoder is fed with the words in the target sequence\n",
    "        if teacher_forcing:\n",
    "            # returns a tensor of shape (max_out_seq_length - 1, batch_size) \n",
    "            # that contains all the target sequences in the batch, \n",
    "            # except the last token of each sequence. \n",
    "            # This is often used in Seq2Seq models, where the decoder inputs are \n",
    "            # typically the target sequences shifted one time step to the left, so that \n",
    "            # the model learns to predict the next token in the sequence based on the previous tokens. The last token of each target sequence is typically not used as input to the decoder, since there is no \"next\" token to predict after it.\n",
    "            shifted_pad_tgt_seqs = pad_tgt_seqs[:-1]\n",
    "            # concatenate the max_out_seq_length dimension withcommon batch_size\n",
    "            y = torch.cat((SOS, shifted_pad_tgt_seqs), dim=0)\n",
    "            y = self.embedding(y)\n",
    "            \n",
    "            # torch.relu is a functional interface to the ReLU activation function. \n",
    "            # It applies the element-wise ReLU function to the input tensor and returns the \n",
    "            # result as a new tensor. torch.relu is a stateless operation, which means that \n",
    "            # it does not have any trainable parameters.\n",
    "\n",
    "            # nn.ReLU is a module that implements the ReLU activation function as a layer \n",
    "            # in a neural network. It can be used as a building block in a neural network, \n",
    "            # and can be trained along with the other layers of the network. nn.ReLU is a \n",
    "            # stateful operation, which means that it has trainable parameters \n",
    "            # (i.e., the weights and biases of the layer).\n",
    "            y = torch.relu(y)\n",
    "            outputs, hidden = self.gru(y, hidden)\n",
    "            outputs = F.log_softmax(self.out(outputs), dim=2)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # At the beginning of decoding, the previous word input is fed with a special word \n",
    "            # SOS which stands for \"start of a sentence\". \n",
    "            # y0 = SOS\n",
    "            y = SOS\n",
    "            max_length = MAX_LENGTH if pad_tgt_seqs is None else len(pad_tgt_seqs)\n",
    "            for i in range(max_length):\n",
    "                # Each cell of the decoder is a GRU, \n",
    "                # it receives as inputs the previous state $z_{i-1}$ and \n",
    "                # relu of the **embedding** of the previous word. \n",
    "\n",
    "                # During training, we know the target sentence for decoding, \n",
    "                # therefore we can feed the correct words $y_i$ as inputs to the recurrent unit.\n",
    "                y = torch.LongTensor(y)\n",
    "                y = self.embedding(y)\n",
    "                y = torch.relu(y)\n",
    "                \n",
    "                # $z_0$ is the output of the encoder, that is $z_0 = h_5$, \n",
    "                # thus `hidden_size` of the decoder should be the same as `hidden_size` of the encoder.\n",
    "                # $z_{i}$ is mapped to $y_{i}$ using a linear layer `self.out` \n",
    "                # followed by `F.log_softmax` \n",
    "                # (because we use `nn.NLLLoss` loss for training).\n",
    "                z, hidden = self.gru(y, hidden)\n",
    "                z = self.out(z)\n",
    "                # $y_{i}$ are the log-probabilities of the words in the target language, \n",
    "                # the dimensionality of $y_{i}$ is the size of the target dictionary.\n",
    "                y = F.log_softmax(z, dim=2)\n",
    "                # concatenate the probabilities of the target words \n",
    "                # Note that the decoder outputs a word at every step and \n",
    "                # thus, you need to embed the words of the target language as well. \n",
    "                outputs = y if i == 0 else torch.cat((outputs,y), dim=0)\n",
    "                # The previous word is taken as the word with the maximum log-probability.\n",
    "                y = torch.argmax(y, dim=2)\n",
    "        \n",
    "        # Do not forget to transfer tensors that you may want to create in this function \n",
    "        # to the device specified by `hidden.device`\n",
    "        outputs.to(hidden.device)\n",
    "        hidden.to(hidden.device)\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb5dd456e3e629a59f81291bfb10586a",
     "grade": false,
     "grade_id": "cell-f0749cc463edb855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Decoder_shapes():\n",
    "    hidden_size = 2\n",
    "    tgt_dictionary_size = 5\n",
    "    test_decoder = Decoder(tgt_dictionary_size, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    pad_tgt_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])  # [max_seq_length, batch_size]\n",
    "\n",
    "    hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=False)\n",
    "\n",
    "    assert outputs.size(0) <= 4, f\"Too long output sequence: outputs.size(0)={outputs.size(0)}\"\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=True)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad shape outputs.shape={outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    # Generation mode\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, None, teacher_forcing=False)\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "559bce0c1449095817506cd0df28f98e",
     "grade": true,
     "grade_id": "test_Decoder",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests Decoder\n",
    "def test_Decoder_no_forcing(Decoder):\n",
    "    # Test without teaching_forcing\n",
    "    with torch.no_grad():\n",
    "        net = Decoder(tgt_dictionary_size=5, embed_size=2, hidden_size=2)\n",
    "        tests.set_weights_decoder(net)\n",
    "\n",
    "        pad_target_seqs = torch.tensor([\n",
    "            [1, 2],\n",
    "            [2, 3],\n",
    "            [3, 0],\n",
    "            [4, 0]\n",
    "        ])\n",
    "\n",
    "        hidden = torch.tensor([\n",
    "            [1., -1.],\n",
    "            [1., -1.],\n",
    "        ]).view(1, 2, 2)\n",
    "        outputs, new_hidden = net.forward(hidden, pad_target_seqs, teacher_forcing=False)\n",
    "        outputs, new_hidden = outputs.float(), new_hidden.float()\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
    "            [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
    "            [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
    "            [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007]\n",
    "        ])\n",
    "\n",
    "        print('outputs[:, 0, :]:\\n', outputs[:, 0, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,0,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        print('outputs[:, 1, :]:\\n', outputs[:, 1, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,1,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        print('Success')\n",
    "\n",
    "test_Decoder_no_forcing(Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b859d602c55a5fcf368af85a719883",
     "grade": true,
     "grade_id": "cell-056d8c00b620337b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.5906, -1.5591, -1.5397, -1.7299, -1.6393],\n",
      "        [-1.6109, -1.5899, -1.5668, -1.6664, -1.6159]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.5906, -1.5591, -1.5397, -1.7299, -1.6393],\n",
      "        [-1.6109, -1.5899, -1.5668, -1.6664, -1.6159]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5091, -1.4770, -1.5174, -1.8772, -1.7245],\n",
      "        [-1.5706, -1.5460, -1.5476, -1.7424, -1.6548],\n",
      "        [-1.6209, -1.5961, -1.5623, -1.6618, -1.6087]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5091, -1.4770, -1.5174, -1.8772, -1.7245],\n",
      "        [-1.5706, -1.5460, -1.5476, -1.7424, -1.6548],\n",
      "        [-1.6209, -1.5961, -1.5623, -1.6618, -1.6087]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests Decoder\n",
    "def test_Decoder_with_forcing(Decoder):\n",
    "    # Test with teaching_forcing\n",
    "    with torch.no_grad():\n",
    "        net = Decoder(tgt_dictionary_size=5, embed_size=2, hidden_size=2)\n",
    "        tests.set_weights_decoder(net)\n",
    "\n",
    "        pad_target_seqs = torch.tensor([\n",
    "            [1, 2],\n",
    "            [2, 3],\n",
    "            [3, 0],\n",
    "            [4, 0]\n",
    "        ])\n",
    "\n",
    "        hidden = torch.tensor([\n",
    "            [1., -1.],\n",
    "            [1., -1.],\n",
    "        ]).view(1, 2, 2)\n",
    "        outputs, new_hidden = net.forward(hidden, pad_target_seqs, teacher_forcing=True)\n",
    "        outputs, new_hidden = outputs.float(), new_hidden.float()\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
    "            [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
    "            [-1.5906, -1.5591, -1.5397, -1.7299, -1.6393],\n",
    "            [-1.6109, -1.5899, -1.5668, -1.6664, -1.6159]\n",
    "        ])\n",
    "        print('outputs[:, 0, :]:\\n', outputs[:, 0, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,0,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
    "            [-1.5091, -1.4770, -1.5174, -1.8772, -1.7245],\n",
    "            [-1.5706, -1.5460, -1.5476, -1.7424, -1.6548],\n",
    "            [-1.6209, -1.5961, -1.5623, -1.6618, -1.6087]\n",
    "        ])\n",
    "        print('outputs[:, 1, :]:\\n', outputs[:, 1, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,1,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        print('Success')\n",
    "\n",
    "test_Decoder_with_forcing(Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "437315da24a76038a6019b7e27f0ce58",
     "grade": true,
     "grade_id": "cell-c5cc115725c6508b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[:, 0, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007],\n",
      "        [-1.6417, -1.6201, -1.5757, -1.6212, -1.5899],\n",
      "        [-1.6459, -1.6282, -1.5851, -1.6042, -1.5852],\n",
      "        [-1.6476, -1.6321, -1.5904, -1.5956, -1.5831],\n",
      "        [-1.6541, -1.6379, -1.5912, -1.5881, -1.5782],\n",
      "        [-1.6571, -1.6408, -1.5919, -1.5842, -1.5759],\n",
      "        [-1.6585, -1.6421, -1.5924, -1.5822, -1.5748]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007],\n",
      "        [-1.6417, -1.6201, -1.5757, -1.6212, -1.5899],\n",
      "        [-1.6459, -1.6282, -1.5851, -1.6042, -1.5852],\n",
      "        [-1.6476, -1.6321, -1.5904, -1.5956, -1.5831],\n",
      "        [-1.6541, -1.6379, -1.5912, -1.5881, -1.5782],\n",
      "        [-1.6571, -1.6408, -1.5919, -1.5842, -1.5759],\n",
      "        [-1.6585, -1.6421, -1.5924, -1.5822, -1.5748]])\n",
      "outputs[:, 1, :]:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007],\n",
      "        [-1.6417, -1.6201, -1.5757, -1.6212, -1.5899],\n",
      "        [-1.6459, -1.6282, -1.5851, -1.6042, -1.5852],\n",
      "        [-1.6476, -1.6321, -1.5904, -1.5956, -1.5831],\n",
      "        [-1.6541, -1.6379, -1.5912, -1.5881, -1.5782],\n",
      "        [-1.6571, -1.6408, -1.5919, -1.5842, -1.5759],\n",
      "        [-1.6585, -1.6421, -1.5924, -1.5822, -1.5748]])\n",
      "expected:\n",
      " tensor([[-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
      "        [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
      "        [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
      "        [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007],\n",
      "        [-1.6417, -1.6201, -1.5757, -1.6212, -1.5899],\n",
      "        [-1.6459, -1.6282, -1.5851, -1.6042, -1.5852],\n",
      "        [-1.6476, -1.6321, -1.5904, -1.5956, -1.5831],\n",
      "        [-1.6541, -1.6379, -1.5912, -1.5881, -1.5782],\n",
      "        [-1.6571, -1.6408, -1.5919, -1.5842, -1.5759],\n",
      "        [-1.6585, -1.6421, -1.5924, -1.5822, -1.5748]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests Decoder\n",
    "def test_Decoder_generation(Decoder):\n",
    "    # Test in generation mode\n",
    "    with torch.no_grad():\n",
    "        net = Decoder(tgt_dictionary_size=5, embed_size=2, hidden_size=2)\n",
    "        tests.set_weights_decoder(net)\n",
    "\n",
    "        hidden = torch.tensor([\n",
    "            [1., -1.],\n",
    "            [1., -1.],\n",
    "        ]).view(1, 2, 2)\n",
    "        outputs, new_hidden = net.forward(hidden, None, teacher_forcing=False)\n",
    "        outputs, new_hidden = outputs.float(), new_hidden.float()\n",
    "\n",
    "        expected = torch.tensor([\n",
    "            [-1.3788, -1.3503, -1.5045, -2.1493, -1.8954],\n",
    "            [-1.5265, -1.4880, -1.5090, -1.8655, -1.7096],\n",
    "            [-1.6097, -1.5715, -1.5319, -1.7189, -1.6249],\n",
    "            [-1.6317, -1.6037, -1.5593, -1.6543, -1.6007],\n",
    "            [-1.6417, -1.6201, -1.5757, -1.6212, -1.5899],\n",
    "            [-1.6459, -1.6282, -1.5851, -1.6042, -1.5852],\n",
    "            [-1.6476, -1.6321, -1.5904, -1.5956, -1.5831],\n",
    "            [-1.6541, -1.6379, -1.5912, -1.5881, -1.5782],\n",
    "            [-1.6571, -1.6408, -1.5919, -1.5842, -1.5759],\n",
    "            [-1.6585, -1.6421, -1.5924, -1.5822, -1.5748]\n",
    "        ])\n",
    "        print('outputs[:, 0, :]:\\n', outputs[:, 0, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,0,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        print('outputs[:, 1, :]:\\n', outputs[:, 1, :])\n",
    "        print('expected:\\n', expected)\n",
    "        assert torch.allclose(outputs[:,1,:], expected, atol=1e-4), \"outputs do not match expected values\"\n",
    "\n",
    "        print('Success')\n",
    "\n",
    "test_Decoder_generation(Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876b987bce981f6692a1847ba12a888f",
     "grade": false,
     "grade_id": "cell-6207d3c96c443b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Training of sequence-to-sequence model using mini-batches\n",
    "\n",
    "Now we are going to train the sequence-to-sequence model on the toy translation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a4c7142fd6d3aea8ec9bd599f32b68",
     "grade": false,
     "grade_id": "cell-dc6ed9b2b10473c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the seq2seq model\n",
    "hidden_size = embed_size = 256\n",
    "encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529e79ae57187958b239781c17b99635",
     "grade": false,
     "grade_id": "cell-6ce059e5ee375d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31e5cb3f3cda9dac28a5d1562df92e24",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Implement the training loop in the cell below. In the training loop, we first encode source sequences using the encoder, then we decode the encoded state using the decoder. The decoder outputs log-probabilities of words in the target language. We need to use these log-probabilities and the indexes of the words in the target sequences to compute the loss.\n",
    "\n",
    "The loss is\n",
    "\\begin{align*}\n",
    "L = - \\frac{1}{N} \\sum_{n} \\sum_{t=1}^{T_n}\n",
    "\\log p\\left(\\mathbf{y}_t^{(n)} \\:\\Bigl|\\: \\mathbf{y}_{<t}^{(n)}, \\mathbf{X}^{(n)} \\right)\n",
    "\\end{align*}\n",
    "where $T_n$ is the length of the $n$-th target sequence and $N= \\sum_{n=1} T_n$ is the total number of words in all the sentences of the mini-batch.\n",
    "\n",
    "Recommended hyperparameters:\n",
    "- Encoder optimizer: Adam with learning rate 0.001\n",
    "- Decoder optimizer: Adam with learning rate 0.001\n",
    "- Number of epochs: 30\n",
    "- Toggle `teacher_forcing` on and off (for each mini-batch) according to the `teacher_forcing_ratio` specified above.\n",
    "\n",
    "Hints:\n",
    "- Training should proceed relatively fast.\n",
    "- If you do well, the training loss should reach 0.1 in 30 epochs.\n",
    "- Slight overlearning may happen (you can see that if you track the test error during training) but you can ignore this problem. \n",
    "- **Important:** When computing the loss, you need to ignore the padded values. This can easily be done by using argument `ignore_index` of function [`nll_loss`](\n",
    "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a530b833c54e4e7de2bac7b3b235c0",
     "grade": false,
     "grade_id": "cell-39d518fd8074b9ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    # Encoder optimizer: Adam with learning rate 0.001\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr = 0.001)\n",
    "    # Decoder optimizer: Adam with learning rate 0.001\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr = 0.001)\n",
    "    \n",
    "    mean_training_errors = []\n",
    "    \n",
    "    # Number of epochs: 30\n",
    "    epochs = 30\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_errors = []\n",
    "        for src_sentence, length, tgt_sentence in trainloader:\n",
    "            encoder_optimizer.zero_grad()\n",
    "            hidden = encoder.init_hidden(len(length))\n",
    "            # In the training loop, we first encode source sequences using the encoder\n",
    "            _, hidden = encoder.forward(src_sentence,length,hidden)\n",
    "            \n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # Toggle teacher_forcing on and off (for each mini-batch) according to the teacher_forcing_ratio specified above\n",
    "            teacher_forcing = np.random.randn() < teacher_forcing_ratio\n",
    "            \n",
    "            # then we decode the encoded state using the decoder. \n",
    "            # The decoder outputs log-probabilities of words in the target language. \n",
    "            # We need to use these log-probabilities and the indexes of the words in \n",
    "            # the target sequences to compute the loss.\n",
    "            outputs, _ = decoder.forward(hidden,tgt_sentence,teacher_forcing)\n",
    "            outputs = torch.flatten(outputs,end_dim=1)\n",
    "            \n",
    "            # When computing the loss, you need to ignore the padded values. \n",
    "            # This can easily be done by using argument ignore_index of function nll_loss\n",
    "            loss = F.nll_loss(outputs,tgt_sentence.view(-1),ignore_index=PADDING_VALUE)\n",
    "            \n",
    "            training_errors.append(loss.item())\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "        mean_training_errors.append(np.mean(training_errors))\n",
    "        print(f\"Epoch {epoch} mean training error: {mean_training_errors[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '1_rnn_encoder.pth', confirm=False)\n",
    "    tools.save_model(decoder, '1_rnn_decoder.pth', confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f02c333feda1857a71e152a65a1d53",
     "grade": false,
     "grade_id": "cell-cb9033683841ebab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 1_rnn_encoder.pth.\n",
      "Model loaded from 1_rnn_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    hidden_size = 256\n",
    "    encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(encoder, '1_rnn_encoder.pth', device)\n",
    "    \n",
    "    decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(decoder, '1_rnn_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3ba6e63d83529028b6a4aab2ee7d9f",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96a617d060e534a0f6981e6e7c20d00b",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Next we need to implement a function that converts input sequences to output sequences using the trained sequence-to-sequence model.\n",
    "\n",
    "Notes:\n",
    "* Since we do not need to compute the gradients in the evaluation phase, we can speed up the computations by using the statement `with torch.no_grad():`.\n",
    "* Please transfer the tensors to `device` inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a3ce2d2714284bbe39dc0dd879f3478",
     "grade": false,
     "grade_id": "cell-1d90b47e7cbf1cc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "      src_seq_lengths: List of source sequence lengths.\n",
    "    \n",
    "    Returns:\n",
    "      out_seqs of shape (MAX_LENGTH, batch_size): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    torch.no_grad()\n",
    "    hidden = encoder.init_hidden(len(src_seq_lengths))\n",
    "    _, hidden = encoder.forward(pad_src_seqs,src_seq_lengths,hidden)\n",
    "    out_seqs, hidden = decoder.forward(hidden,None,False)\n",
    "    out_seqs = torch.argmax(out_seqs,dim=2)\n",
    "    return out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c459aea453b88e64d5be88edbe651f95",
     "grade": false,
     "grade_id": "cell-149ce7a6eeca1021",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([MAX_LENGTH, 2]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a509243e324c5111aaf064a0aaf64b56",
     "grade": false,
     "grade_id": "cell-caa93c3b88e0d8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now translate a few sentences from the training set and print the source, target, and produced output.\n",
    "\n",
    "If you trained the model well enough, the model should memorize the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98212370873fb2a1bc0d0189cbc6ea36",
     "grade": false,
     "grade_id": "cell-444ba3ba61c6db91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def seq_to_tokens(seq, lang):\n",
    "    'Convert a sequence of word indices into a list of words (strings).'\n",
    "    sentence = []\n",
    "    for i in seq:\n",
    "        if i == EOS_token:\n",
    "            break\n",
    "        sentence.append(lang.index2word[i.item()])\n",
    "    return(sentence)\n",
    "\n",
    "def seq_to_string(seq, lang):\n",
    "    'Convert a sequence of word indices into a sentence string.'\n",
    "    return(' '.join(seq_to_tokens(seq, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82f902fa3e4af596b501085007b0c419",
     "grade": false,
     "grade_id": "cell-e3bfe231f4ca66d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate training data:\n",
      "-----------------------------\n",
      "SRC: je suis en train d attendre le bus .\n",
      "TGT: i m waiting for the bus .\n",
      "OUT: i m waiting for the bus .\n",
      "\n",
      "SRC: ce sont celles qui veulent s y rendre .\n",
      "TGT: they are the ones who want to go .\n",
      "OUT: they are the ones who want to go .\n",
      "\n",
      "SRC: je suis le premier musicien de la famille .\n",
      "TGT: i am the first musician in my family .\n",
      "OUT: i am the first musician in my family .\n",
      "\n",
      "SRC: je ne suis pas en mal d argent .\n",
      "TGT: i m not pressed for money .\n",
      "OUT: i m not pressed for money .\n",
      "\n",
      "SRC: elle est accoutumee a veiller toute la nuit .\n",
      "TGT: she is used to staying up all night .\n",
      "OUT: she is used to staying up all night .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Translate training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(trainloader))\n",
    "out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(pad_src_seqs[:,i], trainset.input_lang))\n",
    "    print('TGT:', seq_to_string(pad_tgt_seqs[:,i], trainset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], trainset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "249b5575795aa35f76ff5c9122471fe9",
     "grade": false,
     "grade_id": "cell-66cbefeb6952d610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we translate random sentences from the test set. A well-trained model should output sentences that look similar to the target ones. The mistakes are usually done for words that were rare in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc007c19783ab3b0e6f1a8ca7bf2bc5b",
     "grade": false,
     "grade_id": "cell-cd6dbabc07e7e01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = TranslationDataset(data_dir, train=False)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01a04d9631b3e26c2df1dc8d59688a17",
     "grade": false,
     "grade_id": "cell-135c99847babe153",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate test data:\n",
      "-----------------------------\n",
      "SRC: il est toujours en train de se plaindre .\n",
      "TGT: he is constantly complaining .\n",
      "OUT: he is walking complaining .\n",
      "\n",
      "SRC: je suis ravi que vous ayez souleve ca .\n",
      "TGT: i m glad you brought that up .\n",
      "OUT: i m glad you brought that up .\n",
      "\n",
      "SRC: elle est entierement devouee a ses trois enfants .\n",
      "TGT: she is devoted to her three children .\n",
      "OUT: she is anxious to speak up with .\n",
      "\n",
      "SRC: je suis ravi que tu aies souleve ca .\n",
      "TGT: i m glad you brought that up .\n",
      "OUT: i m glad you brought that up .\n",
      "\n",
      "SRC: tu n es pas aussi maligne que moi .\n",
      "TGT: you re not as smart as me .\n",
      "OUT: you re not as smart as me .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Translate test data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(testloader))\n",
    "out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(pad_src_seqs[:,i], testset.input_lang))\n",
    "    print('TGT:', seq_to_string(pad_tgt_seqs[:,i], testset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], testset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1877152a2cc6ec421c5624c1220464f3",
     "grade": false,
     "grade_id": "cell-1cfd09f0808e511c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Compute BLEU score\n",
    "\n",
    "Let us now compute the [BLEU score](https://en.wikipedia.org/wiki/BLEU) for the translations produced by our model. We can use the PyTorch function [bleu_score](https://pytorch.org/text/stable/data_metrics.html?highlight=bleu_score#torchtext.data.metrics.bleu_score) for that.\n",
    "\n",
    "* **Your model should achieve a minimum BLEU score of 90 on the training set.**\n",
    "* The BLEU score on the test set should be greater than 40.\n",
    "\n",
    "The model can severly overfit to the training set and we do not cope with the overfitting problem in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d6a35b286e16218271c80309237a07",
     "grade": false,
     "grade_id": "cell-c7c83e4783126f91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14071c9e20b3ed2969814459ebf12f9",
     "grade": true,
     "grade_id": "cell-d9e870bdbb7a77aa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 96.95085883140564\n"
     ]
    }
   ],
   "source": [
    "# Create translations for the training set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for pad_src_seqs, src_seq_lengths, pad_tgt_seqs in trainloader:\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, trainset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, trainset.output_lang)] for seq in pad_tgt_seqs.T])\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "assert score*100 > 90, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "156c000eef2a4d8dfcb638e3d77403d8",
     "grade": true,
     "grade_id": "cell-6960c91a78696fe7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on test data: 47.462530897787516\n"
     ]
    }
   ],
   "source": [
    "# Create translations for the test set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for pad_src_seqs, src_seq_lengths, pad_tgt_seqs in testloader:\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, testset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, testset.output_lang)] for seq in pad_tgt_seqs.T])\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on test data: {score*100}')\n",
    "assert score*100 > 40, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bb3216ab2284b419d084dd20e6b6283",
     "grade": false,
     "grade_id": "cell-3e93ef4953126093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook:\n",
    "* We learned how recurrent neural networks can be used to build a sequence-to-sequence model.\n",
    "* We trained a sequence-to-sequence model for statistical machine translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
