{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99142d41f97264dcc021f5a16611ed44",
     "grade": false,
     "grade_id": "cell-87195ccb7e06731c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 4\n",
    "<br>\n",
    "<b>Deadline:</b> March 10, 2021 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1.2. Train a multilayer perceptron (MLP) network in numpy.\n",
    "\n",
    "In this exercise, we implement training of a multilayer perceptron network using the `numpy` library.\n",
    "* We implement forward and backward computations required for computing the gradients with backpropagation.\n",
    "* We train an MLP on a toy data set.\n",
    "\n",
    "We will implement an MLP with two hidden layers like shown in this figure:\n",
    "\n",
    "<img src=\"mlp.png\" width=300 style=\"float: top;\">\n",
    "\n",
    "We will build the following computational graph:\n",
    "\n",
    "<img src=\"mlp_compgraph.png\" width=600 style=\"float: top;\">\n",
    "\n",
    "Note that the computational graph contains a mean-squared error (MSE) loss because we solve a regression problem.\n",
    "\n",
    "Recall what we discussed in the lecture:\n",
    "\n",
    "<img src=\"backprop_software.png\" width=800 style=\"float: top;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eed5501920d3e124c40a66dcee99fa7",
     "grade": false,
     "grade_id": "cell-cafdead5e95c3773",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe00604c3ac013b22df1df7f9c7f175c",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During grading, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d86acc1148120d049457fdc7439961d",
     "grade": false,
     "grade_id": "cell-1b57af4f7c548374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Gradient of the loss\n",
    "\n",
    "We start by implementing the last block of the computational graph which is the mean-squared error loss:\n",
    "$$\n",
    "c = \\frac{1}{N} \\sum_{i=1}^N (y_i - t_i)^2\n",
    "$$\n",
    "where $y_i$ are the elements of an input vector $\\mathbf{y}$ and $t_i$ are the elements of the target vector $\\mathbf{t}$.\n",
    "\n",
    "In the code below, we define a class that performs forward and backward computations of this loss function. Your task is to implement the `backward` function which should compute the gradient $\\frac{\\partial c}{\\partial \\mathbf{y}}$.\n",
    "\n",
    "Note that we process all $N$ training examples at the same time. Therefore, our implementation operates with two-dimensional arrays of shape `(n_samples, some_size)` where `n_samples` is the number $N$ of training samples and `some_size` is the size of an intermediate vector produced inside the MLP (e.g., the number of neurons in a hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9559a39ad6bbb19c80e6001054fae18d",
     "grade": false,
     "grade_id": "MSELoss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y of shape (n_samples, ysize): Inputs of the loss function (can be, e.g., an output of a neural network).\n",
    "          target of shape (n_samples, ysize): Targets.\n",
    "        \n",
    "        Returns:\n",
    "          loss (float): The loss value.\n",
    "        \"\"\"\n",
    "        self.diff = diff = y - target  # Keep this for backward computations\n",
    "        c = np.sum(np.square(diff)) / diff.size\n",
    "        return c\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          dy of shape (n_samples, ysize): Gradient of the MSE loss wrt the inputs.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'diff'), \"Need to call forward() first\"\n",
    "        # YOUR CODE HERE\n",
    "        dy = 2*self.diff / self.diff.size\n",
    "        return dy\n",
    "        # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b193d7a9befef960baae9a6809382fba",
     "grade": false,
     "grade_id": "cell-ea0f3206867f3d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MSELoss_shapes():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "    dy = loss.backward()  # Do backward computations\n",
    "    assert dy.shape == y.shape, f\"Bad dy.shape: {dy.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50b49f6bef0d311048eb9019ffb4e770",
     "grade": false,
     "grade_id": "cell-c1c40dfe278fb3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can test our implementation by comparing the results of our backward computations with a [numerical estimate](https://en.wikipedia.org/wiki/Numerical_differentiation) of the gradient: Suppose we have function \n",
    "$f(\\mathbf{x})$ of a vector input $\\mathbf{x}$, then the gradient can be estimated numerically at a (randomly chosen) input $\\mathbf{x}$ by\n",
    "$$ \\nabla f(\\mathbf{x}) \\approx \\frac{f(\\mathbf{x} + \\epsilon) - f(\\mathbf{x} - \\epsilon)}{2\\epsilon}$$\n",
    "using small $\\epsilon$. Note that the numerical gradient is an approximation of the analytical one and therefore there will be a small numerical difference between them.\n",
    "\n",
    "The function that we import in the cell below implements numerical computations of the gradient of a given function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdf411c2a8dd9bb465f4c00c7352cc27",
     "grade": false,
     "grade_id": "cell-2561d9e518b3b4bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8824a44edc5561f43ce33c8e2a9c29f",
     "grade": false,
     "grade_id": "cell-ee82c925fae51626",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [ 0.68584264 -0.29082003  0.29915849]\n",
      "Numerical gradient:\n",
      " [ 0.68584264 -0.29082003  0.29915849]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We now compare our analytical computations of the gradient with its numerical estimate\n",
    "def test_MSELoss_backward():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "\n",
    "    dy = loss.backward()\n",
    "    print('Analytical gradient:\\n', dy)\n",
    "    dy_num = numerical_gradient(lambda y: loss.forward(y, target), y)\n",
    "    print('Numerical gradient:\\n', dy_num[0])\n",
    "    assert np.allclose(dy, dy_num), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a24dff52a0e9e23c15ebd791e6e6605",
     "grade": true,
     "grade_id": "cell-c8af99b85700fd63",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d33f0fd41d4e3918c600dfe8f788fad",
     "grade": false,
     "grade_id": "cell-b71f50dda717743d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Linear layer\n",
    "\n",
    "Next we implement a linear layer.\n",
    "\n",
    "The forward computations of the linear layer are\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In the backward pass, the linear layer receives the gradients wrt to the outputs $\\frac{\\partial c}{\\partial \\mathbf{y}}$ and it needs to compute:\n",
    "* the gradients wrt the layer parameters $\\mathbf{W}$ and $\\mathbf{b}$\n",
    "* the gradient $\\frac{\\partial c}{\\partial \\mathbf{x}}$ wrt the inputs.\n",
    "\n",
    "In the cell below, we define a class that resembles class [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear) of pytorch. It calls functions `linear_forward` and `linear_backward` that implement the forward and backward computations. We implmented it this way because it makes it easier to test `linear_forward` and `linear_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f461dc0f31ffcad9657dc0e1427f57b8",
     "grade": false,
     "grade_id": "cell-422a5f3dad7faf7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize the weights\n",
    "        bound = 3 / np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-bound, bound, (out_features, in_features))\n",
    "        bound = 1 / np.sqrt(in_features)\n",
    "        self.b = np.random.uniform(-bound, bound, out_features)\n",
    "\n",
    "        self.grad_W = None  # Attribute to store the gradients wrt W\n",
    "        self.grad_b = None  # Attribute to store the gradients wrt b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_samples, xsize): Inputs\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (n_samples, ysize): Outputs of shape.\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy of shape (n_samples, ysize): Gradient of a loss wrt outputs.\n",
    "        \n",
    "        Returns:\n",
    "          dx of shape (n_samples, xsize): Gradient of a loss wrt inputs.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first\"\n",
    "        assert dy.ndim == 2 and dy.shape[1] == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52c10d9d190c43559fe2ed272ac34dbb",
     "grade": false,
     "grade_id": "cell-c7ec54dadaa14e7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your task is to implement `linear_forward` and `linear_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39b4025a5ecd7d5ffe8fc338dc42061b",
     "grade": false,
     "grade_id": "cell-92fa2567f6455189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(x, W, b):\n",
    "    \"\"\"Forward computations in the linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x of shape (n_samples, xsize): Inputs .\n",
    "      W of shape (ysize, xsize): Weight matrix.\n",
    "      b of shape (ysize,): Bias term.\n",
    "\n",
    "    Returns:\n",
    "      y of shape (n_samples, ysize): Outputs.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #print('LIN_f_in (x,W,b):', x.shape, W.shape, b.shape)\n",
    "    y = np.add(np.matmul(x,W.T),b)\n",
    "    #print('LIN_f_out (y):', y.shape)\n",
    "    return y\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    \n",
    "def linear_backward(dy, x, W, b):\n",
    "    \"\"\"Backward computations in the linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy of shape (n_samples, ysize): Gradient of a loss wrt outputs.\n",
    "      x of (n_samples, xsize): Input of shape.\n",
    "      W of shape (ysize, xsize): Weight matrix.\n",
    "      b of shape (ysize,): Bias term.\n",
    "\n",
    "    Returns:\n",
    "      dx of shape (n_samples, xsize): Gradient of a loss wrt inputs.\n",
    "      dW of shape (ysize, xsize): Gradient wrt weight matrix W.\n",
    "      db of shape (ysize,): Gradient wrt bias term b.\n",
    "    \"\"\"\n",
    "    assert dy.ndim == 2 and dy.shape[1] == W.shape[0]\n",
    "    # YOUR CODE HERE\n",
    "    dx = np.matmul(dy,W)\n",
    "    dW = np.matmul(dy.T,x)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dx, dW, db\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e75506b30da32b7fa723a95c25a7b7c",
     "grade": false,
     "grade_id": "cell-7d17ae70e7b767ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We test the shapes of the outputs\n",
    "def test_linear_shapes():\n",
    "    n_samples = 4\n",
    "    x = np.random.randn(n_samples, 2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    # Test shapes\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(n_samples * 3).reshape((n_samples, 3))\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    assert dW.shape == W.shape, f\"Bad dW.shape={dW.shape}, W.shape={W.shape}\"\n",
    "    assert db.shape == b.shape, f\"Bad db.shape={db.shape}, b.shape={b.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_linear_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0675e30663ed9f4668cac970c31694ca",
     "grade": false,
     "grade_id": "cell-1f307ba68557823b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can again test the backward computations by numerical differentiation.\n",
    "\n",
    "Note that function `numerical_gradient` imported above accepts functions `fun` that works only with *one-dimensional arrays* as inputs and outputs.\n",
    "\n",
    "Suppose we have function `fun(X)` which accepts a two-dimensional array `X` of shape `(n1, n2)` as input and produces a one-dimensional array `y` of shape `(ny,)` as output. We want to compute partial derivatives\n",
    "`d y[i] / d X[k,l]` for each output element `y[i]` and each element `X[k,l]` of the input matrix. We can to it in the following way.\n",
    "\n",
    "First, we define a function with one-dimensional inputs such that it can be passed to our `numerical_gradient`\n",
    "function. Function `fun2` reshapes a one-dimensional array passed to it and calls function `fun`:\n",
    "```\n",
    "fun2 = lambda A: fun(A.reshape(n1, n2))\n",
    "```\n",
    "\n",
    "Then we can call the `numerical_gradient` function:\n",
    "```\n",
    "A = np.random.randn(n1, n2)\n",
    "dA = numerical_gradient(fun2, A.flatten())\n",
    "```\n",
    "which will produce a two dimensional array of shape `(ny, n1*n2)` that will contain the required partial\n",
    "derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ab42cdea4b6b41dd2fe8cb46a1d873c",
     "grade": false,
     "grade_id": "cell-00b350d6c7ada8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [[ 11.80043164  -9.96913269]\n",
      " [ 12.44911672 -10.25973086]\n",
      " [ 13.0978018  -10.55032903]]\n",
      "Numerical gradient:\n",
      " [[ 11.80043164  -9.96913269]\n",
      " [ 12.44911672 -10.25973086]\n",
      " [ 13.0978018  -10.55032903]]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We test the backward computations of d/dW by numerical differentiation\n",
    "def test_linear_numerically():\n",
    "    n_samples = 4\n",
    "    x = np.random.randn(n_samples, 2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(n_samples * 3).reshape((n_samples, 3))\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "\n",
    "    print('Analytical gradient:\\n', dW)\n",
    "    dW_num = numerical_gradient(lambda W: linear_forward(x, W.reshape(3, 2), b).flatten(), W.flatten())\n",
    "    dW_num = dW_num.reshape(y.shape + W.shape)\n",
    "    expected = (dy[:, :, None, None] * dW_num).sum(axis=(0,1))\n",
    "    print('Numerical gradient:\\n', expected)\n",
    "    assert np.allclose(dW, expected), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_linear_numerically()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4137e6ce36250cf8abafae53b40c87c7",
     "grade": false,
     "grade_id": "cell-91d19db375ecd0d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recommend you to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd9775725d04e74cef35241eb570f99",
     "grade": true,
     "grade_id": "linear_batch_Wb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a77743a622413602365f3d776e550013",
     "grade": true,
     "grade_id": "cell-6bad54818463363a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1316dccf770021830b55f35740560574",
     "grade": true,
     "grade_id": "linear_batch_x",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0e0dbbc7008220c348ff24e28fc9785",
     "grade": false,
     "grade_id": "cell-f6dc372ec175c898",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. The Tanh activation function\n",
    "\n",
    "Next we implement the Tanh activation function. The activation function is applied *element-wise* to input vector $\\mathbf{x}$ to produce outputs $\\mathbf{y}$:\n",
    "$$\n",
    "\\mathbf{y} = \\text{tanh}(\\mathbf{x}) \\quad \\text{such that} \\quad y_i = \\text{tanh}(x_i).\n",
    "$$\n",
    "\n",
    "When we backpropagate through that block, we need to transform the gradients $\\frac{\\partial c}{\\partial \\mathbf{y}}$ wrt to the outputs into the gradients wrt the inputs $\\frac{\\partial c}{\\partial \\mathbf{x}}$. Your task is to implement the forward and backward computations.\n",
    "\n",
    "Notes:\n",
    "* We recommend you to compare analytical and numerical computations of the gradient.\n",
    "* If you use function `numerical_gradient` to differentiate numerically `Tanh.forward()` using a one-dimensional array `x` as input, the output of `numerical_gradient` is a two-dimensional array (Jacobian matrix). We are interested only in the diagonal elements of that array because the nonlinearity is applied *element-wise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620b72703d6c8524017b696f71a8ad85",
     "grade": false,
     "grade_id": "cell-15a2ce86bee8cd17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (n_features,).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self.x = x\n",
    "        return Tanh_forward(x)\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (n_features,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        # YOUR CODE HERE\n",
    "        return Tanh_backward(dy,self.x)\n",
    "        # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh_forward(x):\n",
    "    y = np.tanh(x)\n",
    "    return y\n",
    "\n",
    "def Tanh_backward(dy,x):\n",
    "    dx = dy * (1-np.square(np.tanh(x)))\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d03a6c844168fa1c36d7b63b4ad33a",
     "grade": false,
     "grade_id": "cell-a2624334df2e830f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Tanh_shapes():\n",
    "    x = np.random.randn(3)\n",
    "    act_fn = Tanh()\n",
    "    y = act_fn.forward(x)\n",
    "    dy = np.arange(1, 4)\n",
    "    dx = act_fn.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Tanh_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [0.41989544 0.39298273 1.26708273 3.07005027]\n",
      "Numerical gradient:\n",
      " [0.41989545 0.39298273 1.26708274 3.07005027]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We test the backward computations of d/dx by numerical differentiation\n",
    "def test_tanh_numerically():\n",
    "    n_samples = 4\n",
    "    x = np.random.randn(n_samples)\n",
    "    \n",
    "    dy = np.arange(1,n_samples+1)\n",
    "    dx = Tanh_backward(dy, x)\n",
    "\n",
    "    print('Analytical gradient:\\n', dx)\n",
    "    dx_num = numerical_gradient(Tanh_forward, x)\n",
    "    expected = dy*np.diagonal(dx_num)\n",
    "    print('Numerical gradient:\\n', expected)\n",
    "    assert np.allclose(dx, expected), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_tanh_numerically()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f3e52bc9f308d8c95157d35a24ba623",
     "grade": false,
     "grade_id": "cell-15e28241c83862b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4. Multilayer Perceptron (MLP)\n",
    "\n",
    "In the cell below, you need to implement an MLP with two hidden layers and `Tanh` nonlinearity. Use instances of classes `Linear` and `Tanh` in your implementation.\n",
    "\n",
    "Note:\n",
    "* For testing purposes, the instances of `Linear` and `Tanh` classes should be attributes of class `MLP` such as attribute `fc1` in the example below:\n",
    "```\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        self.fc1 = Linear(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1a778e3751d9cba4470e79d054d4bbb",
     "grade": false,
     "grade_id": "MLPBatch",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of inputs which should be equal to xsize.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = Linear(in_features, hidden_size1)\n",
    "        self.fc2 = Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = Linear(hidden_size2, out_features)\n",
    "        \n",
    "        self.tanh1 = Tanh()\n",
    "        self.tanh2 = Tanh()\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape [N, xsize].\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape [N, ysize].\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        y = self.fc1.forward(x)\n",
    "        y = self.tanh1.forward(y)\n",
    "        y = self.fc2.forward(y)\n",
    "        y = self.tanh2.forward(y)\n",
    "        y = self.fc3.forward(y)\n",
    "        return y\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs (shape [N, ysize]).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs (shape [N, xsize]).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        dx = self.fc3.backward(dy)\n",
    "        dx = self.tanh2.backward(dx)\n",
    "        dx = self.fc2.backward(dx)\n",
    "        dx = self.tanh1.backward(dx)\n",
    "        dx = self.fc1.backward(dx)\n",
    "        return dx\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2652272c9b14ad871a8b564d341ac032",
     "grade": false,
     "grade_id": "cell-07c80ef21983d673",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MLP_shapes():\n",
    "    n_samples = 10\n",
    "    x = np.random.randn(n_samples, 1)\n",
    "    mlp_batch = MLP(1, 10, 20, 1)\n",
    "    y = mlp_batch.forward(x)\n",
    "\n",
    "    dy = np.arange(n_samples).reshape((n_samples, 1))   # Dummy gradient of a loss function wrt MLP's outputs.\n",
    "    dx = mlp_batch.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MLP_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7914368031d1b33de874396143d64051",
     "grade": true,
     "grade_id": "test_MLPBatch",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366fa51dd0dde9f63b4a0fd2a856224f",
     "grade": false,
     "grade_id": "cell-91d50afcfcbc324d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's create an MLP with random weights and compute the derivative wrt the one-dimensional input\n",
    "def test_MLP_derivative():\n",
    "    n_samples = 100\n",
    "    x = np.linspace(-10, 10, n_samples)\n",
    "    mlp_batch = MLP(1, 10, 20, 1)\n",
    "    y = mlp_batch.forward(x.reshape((n_samples, 1))).flatten()\n",
    "\n",
    "    dy_dx = mlp_batch.backward(np.ones((n_samples, 1))).flatten()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.plot(x, dy_dx)\n",
    "    ax.grid(True)\n",
    "    ax.legend(['y', 'dy_dx'])\n",
    "\n",
    "test_MLP_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb865aabe7b04a79776e7f6fed6750f",
     "grade": false,
     "grade_id": "cell-d5136f1291f0c36a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can visually inspect whether the computations of the derivative seem correct.\n",
    "\n",
    "More importantly, we can compute the gradient of a loss wrt the parameters of the MLP. The gradients can be used to update the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "651b3f554cfd6e87e29e2791e44c95e0",
     "grade": false,
     "grade_id": "cell-0630dc5ad992327d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Training MLP network with backpropagation\n",
    "\n",
    "Now let us use our code to train an MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e0b1798b19d46a30fd9585db6867ec8",
     "grade": false,
     "grade_id": "cell-bb746d106b37391b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbce091f670>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us generate toy data\n",
    "np.random.seed(2)\n",
    "x = np.random.randn(100, 1)\n",
    "x = np.sort(x, axis=0)\n",
    "targets = np.sin(x * 2 * np.pi / 3)\n",
    "targets = targets + 0.2 * np.random.randn(*targets.shape)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, targets, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc44ce20da9194528e14024d166764ab",
     "grade": false,
     "grade_id": "cell-b5925a7912f3191c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And train an MLP network using gradient descent\n",
    "from IPython import display\n",
    "\n",
    "if not skip_training:  # The trained MLP is not tested\n",
    "    mlp = MLP(1, 10, 11, 1)  # Create MLP network\n",
    "    loss = MSELoss()  # Create loss\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x, targets, '.')\n",
    "    learning_rate = 0.05\n",
    "    n_epochs = 1 if skip_training else 200\n",
    "    for i in range(n_epochs):\n",
    "        # Forward computations\n",
    "        y = mlp.forward(x)\n",
    "        c = loss.forward(y, targets)\n",
    "\n",
    "        # Backward computations\n",
    "        dy = loss.backward()\n",
    "        dx = mlp.backward(dy)\n",
    "\n",
    "        # Gradient descent update\n",
    "        #learning_rate *= 0.99  # Learning rate annealing\n",
    "        for module in mlp.__dict__.values():\n",
    "            if hasattr(module, 'W'):\n",
    "                module.W = module.W - module.grad_W * learning_rate\n",
    "                module.b = module.b - module.grad_b * learning_rate\n",
    "\n",
    "        ax.clear()\n",
    "        ax.plot(x, targets, '.')\n",
    "        ax.plot(x, y, 'r-')\n",
    "        ax.grid(True)\n",
    "        ax.set_title('Iteration %d/%d' % (i+1, n_epochs))\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        plt.pause(0.005)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "778ff4525fa210606bebf85f115fb080",
     "grade": false,
     "grade_id": "cell-ca3ef83db74fc3f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you implement the MLP correctly, you will see that the learned function fits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "295b11424526bb9dc1ea17bf141803b7",
     "grade": false,
     "grade_id": "cell-841919d678edba70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "Now you have implemented backpropagation and trained an MLP network using gradient descent.\n",
    "\n",
    "PyTorch makes it easier to create neural networks with different architectures and optimize its parameters using (variants of) gradient descent:\n",
    "* It contains multiple building blocks with forward and backward computations implemented.\n",
    "* It implements optimization methods that work well for neural networks.\n",
    "* Computations can be performed either on GPU or CPU using the same code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
