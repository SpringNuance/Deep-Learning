{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c47e66ea12a5aec54214d0bcbfd9ed6",
     "grade": false,
     "grade_id": "cell-9aa58d6cac14c783",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> May 4, 2022 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 2. Generative adversarial networks (GANs). WGAN-GP: Wasserstein GAN with gradient penalty\n",
    "\n",
    "The goal of this exercise is to get familiar with WGAN-GP: one of the most popular versions of GANs, which is relatively easy to train.\n",
    "\n",
    "The algorithm was introduced in the paper [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67c33e7b0ef5b2539c31188a69d90430",
     "grade": true,
     "grade_id": "cell-170e509aea63f9e2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99f956742bb7af044124d3d3c39cf49f",
     "grade": false,
     "grade_id": "cell-4f6ca14d3a2fa27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e95d5df11dc09d17600373a1afce0e80",
     "grade": false,
     "grade_id": "cell-79a0ef10470c37ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We will use MNIST data in this exercise. Note that we re-scale images so that the pixel intensities are in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bde986a0f605eaf0c6abf8f56387d2c0",
     "grade": false,
     "grade_id": "cell-24de0b6a166fd150",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92cd4596cfe7a59c2f48e0afdf5e9092",
     "grade": false,
     "grade_id": "cell-511beddf297bf38e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Wasserstein GAN (WGAN)\n",
    "\n",
    "The WGAN value function is constructed as\n",
    "$$\n",
    "  \\min_g \\max_{d \\in D} E_{x∼P_r}[d(x)] − E_{\\widetilde x∼P_g}[d(\\widetilde{x})]\n",
    "$$\n",
    "where\n",
    "* the discriminator $d$ is constrained to be in a restricted set $D$ of functions\n",
    "* $P_r$ is the data distribution\n",
    "* $P_g$ is the model distribution. Samples from the model distribution are produced as follows:\n",
    "\\begin{align}\n",
    "z &\\sim N(0, I)\n",
    "\\\\\n",
    "\\widetilde{x} &= g(z)\n",
    "\\end{align}\n",
    "\n",
    "## Generator\n",
    "\n",
    "Implement the generator in the cell below. We recommend you to use the same architecture of the generator as in Exercise 11.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33b7265b69d50a1847d4911bdf29ae69",
     "grade": false,
     "grade_id": "Generator",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        \"\"\"WGAN generator.\n",
    "        \n",
    "        Args:\n",
    "          nz:  Number of elements in the latent code.\n",
    "          ngf: Base size (number of channels) of the generator layers.\n",
    "          nc:  Number of channels in the generated images.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.gen_conv1 = nn.ConvTranspose2d(kernel_size = 4, stride = 2,in_channels = nz, out_channels = 4*ngf, bias = False, padding = 0)\n",
    "        self.norm1 = nn.BatchNorm2d(num_features = 4*ngf )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # ConvTranspose2d layer with kernel_size=4, stride=2, 2*ngf output channels, no bias, followed by BatchNorm2d and ReLU\n",
    "        self.gen_conv2 = nn.ConvTranspose2d(kernel_size = 4, stride = 2,in_channels = 4*ngf, out_channels = 2*ngf, bias = False, padding = 1)\n",
    "        self.norm2 = nn.BatchNorm2d(num_features = 2*ngf )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # ConvTranspose2d layer with kernel_size=4, stride=2, ngf output channels, no bias, followed by BatchNorm2d and ReLU\n",
    "        self.gen_conv3 = nn.ConvTranspose2d(kernel_size = 4, stride = 2, in_channels = 2*ngf, out_channels = ngf, bias = False, padding = 2)\n",
    "        self.norm3 = nn.BatchNorm2d(num_features = ngf )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # ConvTranspose2d layer with kernel_size=4, stride=2, nc output channels, no bias, followed by tanh\n",
    "        self.gen_conv4 = nn.ConvTranspose2d(kernel_size = 4, stride = 2, in_channels = ngf, out_channels = nc, bias = False, padding = 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, verbose=False):\n",
    "        \"\"\"Generate images by transforming the given noise tensor.\n",
    "        \n",
    "        Args:\n",
    "          z of shape (batch_size, nz, 1, 1): Tensor of noise samples. We use the last two singleton dimensions\n",
    "              so that we can feed z to the generator without reshaping.\n",
    "          verbose (bool): Whether to print intermediate shapes (True) or not (False).\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (batch_size, nc, 28, 28): Generated images.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if verbose: print(z.size())\n",
    "            \n",
    "        out = self.relu1(self.norm1(self.gen_conv1(z)))\n",
    "        if verbose: print(out.size())\n",
    "            \n",
    "        out = self.relu2(self.norm2(self.gen_conv2(out)))\n",
    "        if verbose: print(out.size())\n",
    "            \n",
    "        out = self.relu3(self.norm3(self.gen_conv3(out)))\n",
    "        if verbose: print(out.size())\n",
    "        \n",
    "        out = self.tanh(self.gen_conv4(out))\n",
    "        if verbose: print(out.size())\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbdd35dde6a4a659769e03d3ccc51ccd",
     "grade": false,
     "grade_id": "cell-53cda167f289ff2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 1, 1])\n",
      "torch.Size([32, 256, 4, 4])\n",
      "torch.Size([32, 128, 8, 8])\n",
      "torch.Size([32, 64, 14, 14])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Generator_shapes():\n",
    "    batch_size = 32\n",
    "    nz = 10\n",
    "    netG = Generator(nz, ngf=64, nc=1)\n",
    "\n",
    "    noise = torch.randn(batch_size, nz, 1, 1)\n",
    "    out = netG(noise, verbose=True)\n",
    "\n",
    "    assert out.shape == torch.Size([batch_size, 1, 28, 28]), f\"Bad out.shape: {out.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Generator_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cebc4c2aca060948d1618764579f8db9",
     "grade": false,
     "grade_id": "cell-f3d7ef6f1dbe76b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss for training the generator\n",
    "\n",
    "The generator is trained to minimize the relevant part of the value function using a fixed discriminator $d$:\n",
    "$$\n",
    "  \\min_g − E_{\\widetilde{x} \\sim P_g}[d(\\widetilde x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4035dd55e7f32ebd7f79fc81ee33cad9",
     "grade": false,
     "grade_id": "cell-a17d362db2f07b09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(netD, fake_images):\n",
    "    \"\"\"Loss computed to train the WGAN generator.\n",
    "\n",
    "    Args:\n",
    "      netD: The discriminator whose forward function takes inputs of shape (batch_size, nc, 28, 28)\n",
    "         and produces outputs of shape (batch_size, 1).\n",
    "      fake_images of shape (batch_size, nc, 28, 28): Fake images produces by the generator.\n",
    "\n",
    "    Returns:\n",
    "      loss: The relevant part of the WGAN value function.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    fake_images = fake_images.to(device)\n",
    "    \n",
    "    out = netD.forward(fake_images).reshape(-1)\n",
    "    \n",
    "    loss = -torch.mean(out)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "711b396b6de45940b5542b5964e741f8",
     "grade": true,
     "grade_id": "cell-e9f124716144c47d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests generator_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6a18a0472bbda332125546f0b464121",
     "grade": false,
     "grade_id": "cell-34a836a2e901a078",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "Implement the WGAN discriminator in the cell below. You can use almost the same architecture as the architecture of the discriminator in Exercise 11.1. The difference is that there is no need to use `sigmoid` nonlinearity in the output layer because the output of the discriminator does not have to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b255c8171293c8edf760b8c85c645ab0",
     "grade": false,
     "grade_id": "cell-eec8fb76f411c0da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc=1, ndf=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          nc:  Number of channels in the images.\n",
    "          ndf: Base size (number of channels) of the discriminator layers.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Conv2d layer with kernel_size=4, stride=2, ndf output channels, no bias, followed by BatchNorm2d and LeakyReLU(0.2)\n",
    "        self.dis_conv1 = nn.Conv2d(in_channels=nc, out_channels=ndf, kernel_size=4, stride=2, bias=False, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(num_features = ndf )\n",
    "        self.relu1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Conv2d layer with kernel_size=4, stride=2, ndf output channels, no bias, followed by BatchNorm2d and LeakyReLU(0.2)\n",
    "        self.dis_conv2 = nn.Conv2d(in_channels=ndf, out_channels=2*ndf, kernel_size=4, stride=2, bias=False, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(num_features = 2*ndf )\n",
    "        self.relu2 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Conv2d layer with kernel_size=4, stride=2, 4*ndf output channels, no bias, followed by BatchNorm2d and LeakyReLU(0.2)\n",
    "        self.dis_conv3 = nn.Conv2d(in_channels=2*ndf, out_channels=4*ndf, kernel_size=4, stride=2, bias=False, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(num_features = 4*ndf )\n",
    "        self.relu3 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Conv2d layer with kernel_size=4, stride=2, 1 output channel, no bias, followed by sigmoid.\n",
    "        self.dis_conv4 = nn.Conv2d(in_channels=4*ndf, out_channels=1, kernel_size=4, stride=2, bias=False, padding=0)\n",
    "\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Images to be evaluated.\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (batch_size,): Discriminator outputs for images x.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if verbose: print(x.size())\n",
    "        \n",
    "        out = self.relu1(self.norm1(self.dis_conv1(x)))\n",
    "        if verbose: print(out.size())\n",
    "        \n",
    "        out = self.relu2(self.norm2(self.dis_conv2(out)))\n",
    "        if verbose: print(out.size())\n",
    "        \n",
    "        out = self.relu3(self.norm3(self.dis_conv3(out)))\n",
    "        if verbose: print(out.size())\n",
    "            \n",
    "        out = self.dis_conv4(out)\n",
    "        if verbose: print(out.size())\n",
    "            \n",
    "        out = torch.squeeze(out)\n",
    "        if verbose: print(out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b25d78cec4efbf2c4a7518e3a7dcff8a",
     "grade": false,
     "grade_id": "cell-44a2221bdef62f26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 64, 14, 14])\n",
      "torch.Size([32, 128, 8, 8])\n",
      "torch.Size([32, 256, 4, 4])\n",
      "torch.Size([32, 1, 1, 1])\n",
      "torch.Size([32])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Discriminator_shapes():\n",
    "    nz = 10  # size of the latent z vector\n",
    "    netD = Discriminator(nc=1, ndf=64)\n",
    "\n",
    "    batch_size = 32\n",
    "    images = torch.ones(batch_size, 1, 28, 28)\n",
    "    out = netD(images, verbose=True)\n",
    "    assert out.shape == torch.Size([batch_size]), f\"Bad out.shape: {out.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Discriminator_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6459a1c82251cafde0152a485a6ea6c2",
     "grade": false,
     "grade_id": "cell-162b94410dda3c54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss for training the WGAN discriminator\n",
    "\n",
    "Recall the value function of WGAN:\n",
    "$$\n",
    "  \\min_g \\max_{d \\in D} E_{x∼P_r}[d(x)] − E_{\\widetilde x∼P_g}[d(\\widetilde x)]\n",
    "$$\n",
    "To tune the discriminator, we need to minimize the following function:\n",
    "$$\n",
    "  \\min_{d \\in D} - E_{x∼P_r}[d(x)] + E_{\\widetilde x∼P_g}[d(\\widetilde x)]\n",
    "$$\n",
    "You need to implement this loss function *assuming no constraints on d* in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7827b5649e38bfc986e20b3756107e07",
     "grade": false,
     "grade_id": "cell-e8349d44426a043a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(netD, real_images, fake_images):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      netD: The discriminator.\n",
    "      real_images of shape (batch_size, nc, 28, 28): Real images.\n",
    "      fake_images of shape (batch_size, nc, 28, 28): Fake images.\n",
    "\n",
    "    Returns:\n",
    "      loss (scalar tensor): Loss for training the WGAN discriminator.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    real_images = real_images.to(device)\n",
    "    fake_images = fake_images.to(device)\n",
    "\n",
    "    real = netD.forward(real_images).reshape(-1)\n",
    "    fake = netD.forward(fake_images).reshape(-1)\n",
    "    \n",
    "    loss = - torch.mean(real) + torch.mean(fake)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd825187ef3947ff37bcfbc14c2d181",
     "grade": true,
     "grade_id": "cell-b697bd01a31143d6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests discriminator_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "136e1bdd3b7b85ae93a37b898c038b57",
     "grade": false,
     "grade_id": "cell-c6bf86344f718387",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Without constraints on $d$, the WGAN value function can be made infinitely large. WGAN constrains the derivative of $d$ using a gradient penalty. The penalty is computed at random points between real images and generated ones using the following procedure:\n",
    "* Given a real image $x$ and a fake image $\\widetilde x$, draw a random number $\\epsilon \\sim U[0,1]$\n",
    "* $\\hat{x} \\leftarrow \\epsilon x + (1−\\epsilon) \\widetilde x$\n",
    "* Compute the gradient penalty $(‖\\nabla_{\\hat{x}} d(\\hat{x})‖_2−1)^2$\n",
    "where $\\nabla_{\\hat{x}} d(\\hat{x})$ is the gradient of $d$ computed at $\\hat{x}$.\n",
    "\n",
    "Your task is to implement the gradient penalty in the cell below.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* We need to compute the gradient $\\nabla d$ so that we can differentiate through the gradient when computing the derivatives wrt the parameters of the discriminator. This can be achieved by using function [torch.autograd.grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) which can create a computational graph with the gradient computations.\n",
    "* The gradient penalty is the average of $(‖\\nabla_{\\hat{x}} d(\\hat{x})‖_2−1)^2$ computed across all samples.\n",
    "* The second output returned by the function is needed for testing your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a819a745ed9e3231d30adff6edc22a20",
     "grade": false,
     "grade_id": "gradient_penalty",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(netD, real, fake_detached):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      netD: The discriminator.\n",
    "      real of shape (batch_size, nc, 28, 28): Real images.\n",
    "      fake_detached of shape (batch_size, nc, 28, 28): Fake images (detached from the computational graph).\n",
    "\n",
    "    Returns:\n",
    "      grad_penalty (scalar tensor): Gradient penalty.\n",
    "      x of shape (batch_size, nc, 28, 28): Points x-hat in which the gradient penalty is computed.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    batch_size = real.shape[0]\n",
    "    nc = real.shape[1]\n",
    "    \n",
    "    eps = torch.rand((batch_size, 1, 1, 1)).repeat(1, nc, 28, 28).to(fake_detached.device)\n",
    "    \n",
    "    x = eps * real + ((1 - eps) * fake_detached)   \n",
    "    x = torch.autograd.Variable(x, requires_grad=True)\n",
    "    \n",
    "    out = netD(x)\n",
    "    \n",
    "    gradients = torch.autograd.grad(inputs=x, outputs=out,\n",
    "                                    grad_outputs=torch.ones_like(out), \n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.shape[0], -1)\n",
    "    gradients_norm = gradients.norm(2, dim=1)\n",
    "    \n",
    "    gradient_penalty = torch.mean((gradients_norm - 1).pow(2))\n",
    "    \n",
    "    return gradient_penalty, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b85fa164cd5e59afa7134752564e6ef",
     "grade": true,
     "grade_id": "test_gradient_penalty",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell test gradient_penalty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00a4d80bed4853a69c03707fcaf5367e",
     "grade": false,
     "grade_id": "cell-beafcb4774140942",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Training WGAN-GP\n",
    "\n",
    "We will now train WGAN-GP. To assess the quality of the generated samples, we will again use the FD score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56ea99e77a2db2a344573ceb7fab79db",
     "grade": false,
     "grade_id": "cell-3f26ac0c61b87f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDScore(\n",
       "  (net): Classifier(\n",
       "    (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (fc1): Linear(in_features=512, out_features=120, bias=True)\n",
       "    (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fd\n",
    "\n",
    "# Load an FD scorer pre-trained on MNIST\n",
    "fdscore = fd.FDScore.pretrained()\n",
    "fdscore.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1062ac61834304eb7fe30aaf410133d3",
     "grade": false,
     "grade_id": "cell-b815a01d40637212",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "nz = 10\n",
    "netG = Generator(nz=nz, ngf=128, nc=1).to(device)\n",
    "netD = Discriminator(nc=1, ndf=128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddb66b43150d45084ff9b13a2cea16ff",
     "grade": false,
     "grade_id": "cell-c11270e33558df93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Optimizer of the discriminator: Adam with learning rate 0.0002 and `betas=(0.5, 0.999)`\n",
    "* Optimizer of the generator:     Adam with learning rate 0.0002 and `betas=(0.5, 0.999)`\n",
    "* Weight $\\lambda=10$ of the gradient penalty term in the discriminator loss:\n",
    "$$\n",
    "  \\min_{d} - E_{x∼P_r}[D(x)] + E_{\\tilde x∼P_g}[d(\\widetilde x)]\n",
    "  + \\lambda (‖\\nabla_{\\hat{x}} d(\\hat{x})‖_2−1)^2\n",
    "$$\n",
    "\n",
    "Hints:\n",
    "- We will use the FD score to assess the quality of the generated samples. The desired level of 10 should be reached after 5-10 epochs. Note that the score is stochastic and it can fluctuate during training. At convergence, the FD score can fluctuate in the range [4, 10].\n",
    "- You can use the following code to track the training progress. The code plots some generated images and computes the score that we use to evaluate the trained model. Note that the images fed to the scorer need to be normalized to be in the range [-1, 1].\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # Plot generated images\n",
    "    z = torch.randn(144, nz, 1, 1, device=device)\n",
    "    samples = netG(z)\n",
    "    tools.plot_generated_samples(samples)\n",
    "    \n",
    "    # Compute score\n",
    "    z = torch.randn(1000, nz, 1, 1, device=device)\n",
    "    samples = netG(z)\n",
    "    score = fdscore.calculate(samples)\n",
    "```\n",
    "- The quality of the images is slightly worse than with the DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae914389933e3986497ab27451391731",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    print('Start training')\n",
    "    # YOUR CODE HERE\n",
    "    lamb = 10\n",
    "    optim_G = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5,0.999))\n",
    "    optim_D = torch.optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5,0.999))\n",
    "    for epoch in range(20):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels.to(device)\n",
    "            iamges = images * 2 - 1 \n",
    "\n",
    "            z = torch.randn(images.size(0), nz, 1, 1, device=device)\n",
    "            gen_images = netG(z)\n",
    "            \n",
    "            optim_D.zero_grad()\n",
    "\n",
    "\n",
    "            penalty, x_hat = gradient_penalty(netD, images, gen_images.detach())\n",
    "            loss = discriminator_loss(netD, images, gen_images.detach()) + lamb * penalty\n",
    "            loss.backward()\n",
    "\n",
    "            optim_D.step()\n",
    "            \n",
    "            optim_G.zero_grad()\n",
    "            \n",
    "            g_loss = generator_loss(netD, gen_images)\n",
    "            g_loss.backward()\n",
    "\n",
    "            optim_G.step()\n",
    "\n",
    "        print('\\nValidationm in Epoch {}:'.format(epoch))\n",
    "        with torch.no_grad():\n",
    "            # Plot generated images\n",
    "            #z = torch.randn(144, nz, 1, 1, device=device)\n",
    "            #samples = netG(z)\n",
    "            #tools.plot_generated_samples(samples)\n",
    "            \n",
    "            # Compute score\n",
    "            z = torch.randn(1000, nz, 1, 1, device=device)\n",
    "            samples = netG(z)\n",
    "            samples = (samples + 1) / 2  # Re-normalize to [0, 1]\n",
    "            score = scorer(samples)\n",
    "        print(score.item())\n",
    "        if epoch > 15 and score > 0.7:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(netG, '2_wgan_g.pth', confirm=True)\n",
    "    tools.save_model(netD, '2_wgan_d.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9618139636a3d0ce802ebaa8034af711",
     "grade": false,
     "grade_id": "cell-542f62dd494b82be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"gen_conv1.weight\", \"norm1.weight\", \"norm1.bias\", \"norm1.running_mean\", \"norm1.running_var\", \"gen_conv2.weight\", \"norm2.weight\", \"norm2.bias\", \"norm2.running_mean\", \"norm2.running_var\", \"gen_conv3.weight\", \"norm3.weight\", \"norm3.bias\", \"norm3.running_mean\", \"norm3.running_var\", \"gen_conv4.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"batch1.weight\", \"batch1.bias\", \"batch1.running_mean\", \"batch1.running_var\", \"batch1.num_batches_tracked\", \"conv2.weight\", \"batch2.weight\", \"batch2.bias\", \"batch2.running_mean\", \"batch2.running_var\", \"batch2.num_batches_tracked\", \"conv3.weight\", \"batch3.weight\", \"batch3.bias\", \"batch3.running_mean\", \"batch3.running_var\", \"batch3.num_batches_tracked\", \"conv4.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m netG \u001b[38;5;241m=\u001b[39m Generator(nz\u001b[38;5;241m=\u001b[39mnz, ngf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m netD \u001b[38;5;241m=\u001b[39m Discriminator(nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ndf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_wgan_g.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tools\u001b[38;5;241m.\u001b[39mload_model(netD, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_wgan_d.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n",
      "File \u001b[0;32m/notebooks/deeplearn2022/081_gan/tools.py:40\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model, filename, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model, filename, device):\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel loaded from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"gen_conv1.weight\", \"norm1.weight\", \"norm1.bias\", \"norm1.running_mean\", \"norm1.running_var\", \"gen_conv2.weight\", \"norm2.weight\", \"norm2.bias\", \"norm2.running_mean\", \"norm2.running_var\", \"gen_conv3.weight\", \"norm3.weight\", \"norm3.bias\", \"norm3.running_mean\", \"norm3.running_var\", \"gen_conv4.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"batch1.weight\", \"batch1.bias\", \"batch1.running_mean\", \"batch1.running_var\", \"batch1.num_batches_tracked\", \"conv2.weight\", \"batch2.weight\", \"batch2.bias\", \"batch2.running_mean\", \"batch2.running_var\", \"batch2.num_batches_tracked\", \"conv3.weight\", \"batch3.weight\", \"batch3.bias\", \"batch3.running_mean\", \"batch3.running_var\", \"batch3.num_batches_tracked\", \"conv4.weight\". "
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    nz = 10\n",
    "    netG = Generator(nz=nz, ngf=128, nc=1)\n",
    "    netD = Discriminator(nc=1, ndf=128)\n",
    "    \n",
    "    tools.load_model(netG, '2_wgan_g.pth', device)\n",
    "    tools.load_model(netD, '2_wgan_d.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d752dca6f2ab976bf2a59c6e6f5193e",
     "grade": false,
     "grade_id": "cell-67853649b178c5ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## GAN evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9c519729d5130ea90259eb2e6991956",
     "grade": true,
     "grade_id": "cell-fa2bdb19fb9b9494",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Save generated samples (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(144, nz, 1, 1, device=device)\n",
    "        samples = netG(z)\n",
    "        torch.save(samples, '2_wgan_samples.pth')\n",
    "else:\n",
    "    samples = torch.load('2_wgan_samples.pth', map_location=lambda storage, loc: storage)\n",
    "\n",
    "tools.plot_generated_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "457c68eae5b16f751d11f6a32350c92e",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the FD score\n",
    "torch.manual_seed(0)\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(1000, nz, 1, 1, device=device)\n",
    "    samples = netG(z)\n",
    "    tools.plot_generated_samples(samples[:144])\n",
    "    score = fdscore.calculate(samples)\n",
    "\n",
    "print(f'FD score: {score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c59be42e8585b06ee4482ee69f1982d7",
     "grade": true,
     "grade_id": "cell-51df347c728031b9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70dc73e72fb92ad36572a63304837921",
     "grade": false,
     "grade_id": "cell-36f4519aec37e741",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook, we learned how to train Wasserstein GAN with gradient penalty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
