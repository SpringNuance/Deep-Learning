{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95f7c987a1aa0cbd3db0c1cdee5c5bbe",
     "grade": false,
     "grade_id": "cell-51463f10ee76cd64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 0\n",
    "<br>\n",
    "<b>Deadline:</b> March 23, 2022 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Regularization techniques\n",
    "\n",
    "This is a simple exercise, in which we will try a few methods to prevent overfitting.\n",
    "\n",
    "## Learning goals:\n",
    "* Understand various methods to prevent overfitting of neural networks.\n",
    "* Experience in using regularization methods in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e7b6f0d864da3ec1bf6ce27ff5f05f6",
     "grade": true,
     "grade_id": "skip_training",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76733d7a70c84a6241a21f0d20ce8bdc",
     "grade": false,
     "grade_id": "cell-8acec5e99baf1c33",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8902a870f19f38ee7fb42e5a40bc3c06",
     "grade": false,
     "grade_id": "cell-55ad6402737821bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6014000b3e03dbc1e2368b986f570d7",
     "grade": false,
     "grade_id": "cell-b4a55be0cb567e24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Regression problem\n",
    "We will look at a regression problem where the task is to estimate a function of one variable\n",
    "$$y = f(x)$$\n",
    "using a set of training examples $(x_1, y_1), \\ldots, (x_n, y_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baeb8a3430abfe96373c944354ee3f75",
     "grade": false,
     "grade_id": "cell-602ebca68e883d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us first generate training examples $y_i=\\sin(x_i) + n_i$ with $x_i$ drawn from the uniform distribution in $[-0.5, 0.5]$ and noise $n_i$ drawn from the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7457ec3683bfb4820fbc6cd583f8d0e",
     "grade": false,
     "grade_id": "cell-b0d3a2083dc9a6af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n = 80\n",
    "x = np.random.rand(n, 1)-0.5\n",
    "\n",
    "def fun(x):\n",
    "    y = np.cos(2* np.pi * x)\n",
    "    y += 0.3 * np.random.randn(*x.shape)\n",
    "    return y\n",
    "\n",
    "y = fun(x)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "083ad76a86a911e9119a37c5e664a35d",
     "grade": false,
     "grade_id": "cell-4ffc8ddcb1221702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us split the data into training, validation and test sets and plot the training and validation sets. And let us plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "408ae376b3c39294ba8280d7ee88a08c",
     "grade": false,
     "grade_id": "cell-293526f20c9d5aa0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "torch.manual_seed(2)\n",
    "rp = torch.randperm(x.size(0))\n",
    "\n",
    "n_train = int(x.size(0) * 0.8)\n",
    "x_train, y_train = x[rp[:n_train]], y[rp[:n_train]]\n",
    "x_test, y_test = x[rp[n_train:]], y[rp[n_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9aaa2d7dd51c8628bdd5e18b6e66fb4",
     "grade": false,
     "grade_id": "cell-c2bebdd68e72cf64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fix, ax = plt.subplots(1)\n",
    "ax.plot(x_train, y_train, 'b.')\n",
    "ax.plot(x_test, y_test, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a85c70f5144240f62108369d9e9111c",
     "grade": false,
     "grade_id": "cell-14cf094e013cff8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Define a multi-layer perceptron (MLP) network with two hidden layers\n",
    "\n",
    "In the code below, we define a neural network architecture with:\n",
    "* input dimension 1\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* linear output layer with output dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "349f64a859ba12da08e789f12acaf6ab",
     "grade": false,
     "grade_id": "cell-5e9ca1bfff94cd3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf69210fc88039355d0704c580f67342",
     "grade": false,
     "grade_id": "cell-915d85fc89b9800e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an MLP network\n",
    "mlp = MLP()\n",
    "mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68c7509e3c44e24aa360515c7ea974b5",
     "grade": false,
     "grade_id": "cell-7f577b3aa66712c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This visualizes the function implemented by an MLP\n",
    "def plot_fit(mlp, x_train, y_train):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x_train, y_train, '.')\n",
    "    x_np = np.linspace(-0.5, 0.5, 100).reshape((-1, 1))\n",
    "    x = torch.tensor(x_np, device=device, dtype=torch.float)\n",
    "    pred = mlp.forward(x).cpu().data.numpy()\n",
    "    ax.plot(x_np, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57a9ed4b12a7abd43ccaa9f5bd5aed24",
     "grade": false,
     "grade_id": "cell-a8e07d60f53a59e0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the function implemented by the MLP\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed9f4819b6fded8bf4fdadb8c6ee32d1",
     "grade": false,
     "grade_id": "cell-54089360cf22ff83",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the function to compute the loss:\n",
    "def compute_loss(mlp, x, y):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        return loss.cpu().numpy()\n",
    "\n",
    "# This is the function to print the progress during training\n",
    "def print_progress(epoch, train_error, val_error):\n",
    "    print('Epoch {}: Train error: {:.4f}, Test error: {:.4f}'.format(\n",
    "        epoch, train_error, val_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0556343b1cc1fb1219188a2ccfa636da",
     "grade": false,
     "grade_id": "cell-38e864109561c7c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Train the MLP network without regularization\n",
    "Training is done by minimizing the mean-squared error computed on the training data:\n",
    "$$c=\\sum_{i=1}^n || f(x_i) - y_i ||^2.$$\n",
    "\n",
    "Here, we train the network:\n",
    "* using all the data for computing the gradient (batch mode)\n",
    "* using `n_epochs` epochs (which is equal to the number of parameter updates in the batch mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9973ba67326d34aa05c5066ee743dcc4",
     "grade": false,
     "grade_id": "cell-6dffb69560f16ba1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34995491e5005bbc59efb54163ab652c",
     "grade": false,
     "grade_id": "cell-f9a61df7e02fe5f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    n_epochs = 10000\n",
    "    train_errors = []  # Keep track of the training data\n",
    "    val_errors = []  # Keep track of the validation data\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 500 == 0:\n",
    "            train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "            val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "            print_progress(epoch, train_errors[-1], val_errors[-1])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_noreg.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLP()\n",
    "    tools.load_model(mlp, 'mlp_noreg.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6caeaedf6af53ff24e63712da170daf",
     "grade": false,
     "grade_id": "cell-2fcb3919234f3be2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a108841c5fbfbb3d4eced5606a0ff05",
     "grade": false,
     "grade_id": "cell-3dbf989a13b25a8a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the network overfits to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9732612495be7078de019a90360a93c",
     "grade": false,
     "grade_id": "cell-982484bc857e80e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss_no_regularization = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss without regularization: %.5f\" % test_loss_no_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bea912b23eccac157f4c82a01bb8856",
     "grade": false,
     "grade_id": "cell-fa92e3b02a5d15c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us look at the learning curves (the evolution of training and test errors during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb77382e7c24532f985d0442d0654c1c",
     "grade": false,
     "grade_id": "cell-f875cdf8b4e10b0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a23c1d74e8e70d25b37ec0bcbbb55f5",
     "grade": false,
     "grade_id": "cell-b6cf5359944d980f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    " As you can see, the test error first decreases and then starts growing. This motivates the following technique to prevent overfitting.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "In early stopping, we stop training when the test error (or validation error) starts growing.\n",
    "\n",
    "In the code below, we define a stopping creterion in function `stop_criterion`. Training is stopped (function returns  `True`) when the validation error is larger than the best validation error obtained so far (with given `tolerance`) for `patience` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "815460d0c24de0c202e39a9b24b3fb2a",
     "grade": false,
     "grade_id": "cell-a16c613b86b9876c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          patience (int):    Maximum number of epochs with unsuccessful updates.\n",
    "          tolerance (float): We assume that the update is unsuccessful if the validation error is larger\n",
    "                              than the best validation error so far plus this tolerance.\n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, val_errors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          val_errors (iterable): Validation errors after every update during training.\n",
    "        \n",
    "        Returns: True if training should be stopped: when the validation error is larger than the best\n",
    "                  validation error obtained so far (with given tolearance) for patience epochs (number of consecutive epochs for which the criterion is satisfied).\n",
    "                 \n",
    "                 Otherwise, False.\n",
    "        \"\"\"\n",
    "        if len(val_errors) <= self.patience:\n",
    "            return False\n",
    "\n",
    "        min_val_error = min(val_errors)\n",
    "        val_errors = np.array(val_errors[-self.patience:])\n",
    "        return all(val_errors > min_val_error + self.tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24a65ebd1ecf0fe9dc89b3d7ec91a91c",
     "grade": false,
     "grade_id": "cell-5e0eae47920604c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the network with early stopping\n",
    "if not skip_training:\n",
    "    n_epochs = 10000\n",
    "    train_errors = []  # Keep track of the training error\n",
    "    val_errors = []  # Keep track of the validation error\n",
    "    early_stop = EarlyStopping(tolerance=0.01, patience=20)\n",
    "\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "\n",
    "        if early_stop.stop_criterion(val_errors):\n",
    "            print(val_errors[epoch])\n",
    "            print('Stop after %d epochs' % epoch)\n",
    "            break\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print_progress(epoch, train_errors[epoch], val_errors[epoch])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_early.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLP()\n",
    "    tools.load_model(mlp, 'mlp_early.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a0d0c2f70d72d1af9085c1794c3146",
     "grade": false,
     "grade_id": "cell-812ee02395587c1d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc478da07030ff2766285548305c865b",
     "grade": false,
     "grade_id": "cell-a4cc056169219a4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5f26864b6cb70d611749f0fc0d7f5c4",
     "grade": false,
     "grade_id": "accuracy_early_stop",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss_early_stopping = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with early stopping: %.5f\" % test_loss_early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d70dd3d3f8e8c2cfb4b55f8c255c03e2",
     "grade": false,
     "grade_id": "cell-898f073dfa615c29",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Weight-decay regularization\n",
    "\n",
    "Let us train the same network with L2 penalties on the weights. In PyTorch, one can add L2 penalty terms for all the parameters by providing `weight_decay` argument for most types of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b03e5008db7552a6dd472638935d1bc3",
     "grade": false,
     "grade_id": "cell-d3cdd57c1eab0c6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train an MLP with L2 regularization\n",
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "del optimizer\n",
    "\n",
    "# Create an Adam optimizer with learning rate 0.01 and weight decay parameter 0.001\n",
    "# optimizer = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0ef355d68931719e125777574eb90e0",
     "grade": false,
     "grade_id": "cell-255cbcade10c4b39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    n_epochs = 4000\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "            val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "            print_progress(epoch, train_errors[-1], val_errors[-1])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_wd.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLP()\n",
    "    tools.load_model(mlp, 'mlp_wd.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37e369446d2c86959069025865c48763",
     "grade": true,
     "grade_id": "weight_decay",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead67ec1f159543e17bde372ac317e43",
     "grade": false,
     "grade_id": "cell-8cd18e4a1f1c9be7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bb8f2b45e601a474fc8fb118c8f7c7b",
     "grade": false,
     "grade_id": "cell-5a77537fb12d0e5c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72037f8c49da2bc1dc570de69f5a6d26",
     "grade": true,
     "grade_id": "accuracy_weight_decay",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss_weight_decay = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with weight decay: %.5f\" % test_loss_weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb5d55e8b48afdaa58af58dbdc754c84",
     "grade": false,
     "grade_id": "cell-838adccf05b5b869",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Injecting noise to inputs\n",
    "\n",
    "One way to improve generalization is to add noise to the inputs. So, we update the parameters of $f$ using the gradient of the following function\n",
    "$$c= \\sum_{i=1}^n || f(x_i + n_i) - y_i ||^2$$\n",
    "where $n_i$ is a noise instance.\n",
    "\n",
    "In the code below, implement adding Gaussian noise with standard deviation to the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b711116823ad36f23d8fd9c9b9c670fe",
     "grade": false,
     "grade_id": "cell-6a44fc25e68a1a04",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def add_noise(x, noise_std):\n",
    "    \"\"\"Add Gaussian noise to a PyTorch tensor.\n",
    "    \n",
    "    Args:\n",
    "      x (tensor): PyTorch tensor of inputs.\n",
    "      noise_std (float): Standard deviation of the Gaussian noise.\n",
    "      \n",
    "    Returns:\n",
    "      x: Tensor with Gaussian noise added.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e873e045a65e74a7bc4b1a64eec01b88",
     "grade": true,
     "grade_id": "noise_injection",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test shapes\n",
    "x = torch.randn(10)\n",
    "x_with_noise = add_noise(x, 0.5)\n",
    "assert x_with_noise.shape == x.shape, f\"Bad x_with_noise.shape: {x_with_noise.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ece66e8ae193e96653c20163fbac9ec",
     "grade": false,
     "grade_id": "cell-1362488ecd1b8214",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75ccecd51a1e791cd45c5e7ec2c524da",
     "grade": false,
     "grade_id": "cell-8fc08479cee992e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train MLP with injecting noise to inputs\n",
    "if not skip_training:\n",
    "    n_epochs = 4000\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(add_noise(x, noise_std=0.05))\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "            val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "            print_progress(epoch, train_errors[-1], val_errors[-1])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_noiseinj.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLP()\n",
    "    tools.load_model(mlp, 'mlp_noiseinj.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3d0d1ed1544bcc8f93a77ee4ce71e5d",
     "grade": false,
     "grade_id": "cell-cbdd3af100896b8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c6bb460c104299c6ebec89b7beecfd6",
     "grade": false,
     "grade_id": "cell-c8eb739eb3ecb6ec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a02bcdf645dee0fe7e246258e1d0969e",
     "grade": true,
     "grade_id": "accuracy_",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_inj_noise = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with noise injection: %.5f\" % test_loss_inj_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80f2d1b6198418e2621924e5e057d5c0",
     "grade": false,
     "grade_id": "cell-b194dea5c4db3b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Another way to improve generalization is to use dropout. In the cell below, define an MLP with exactly the same architecture as previously but with using `nn.Dropout` layers (with dropout probability 0.2) after each `tanh`\n",
    "nonlinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84c650d0725c53ccd403728c33e04e95",
     "grade": false,
     "grade_id": "cell-65cfc4660990f02f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLPDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e0f8405ba34e18f89d8b2f87b3a3743",
     "grade": true,
     "grade_id": "dropout",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPDropout()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd9b85e4b08379f72b6e6777ec031deb",
     "grade": false,
     "grade_id": "cell-2150f522b63cd071",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPDropout()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c021c8447e82b41f5d19f3e5d2feac8",
     "grade": false,
     "grade_id": "cell-abe08aa52fcfcebe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Train MLP with dropout\n",
    "if not skip_training:\n",
    "    #scheduler = StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "    n_epochs = 4000\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #scheduler.step()\n",
    "        mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            mlp.eval()\n",
    "            train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "            val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "            print_progress(epoch, train_errors[-1], val_errors[-1])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_dropout.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLPDropout()\n",
    "    tools.load_model(mlp, 'mlp_dropout.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10892f7bef5c22d4882803179bbe23a8",
     "grade": false,
     "grade_id": "cell-8433adb7942ac189",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c5e115c2e848c0c854db5b45d1454b",
     "grade": false,
     "grade_id": "cell-6c24d16cec138a9c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21e9f1b635f2563eceefcf72a5a04972",
     "grade": true,
     "grade_id": "accuracy_dropout",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_dropout = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with dropout: %.5f\" % test_loss_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd87d02f28cf3e69ce8cc91e556e575d",
     "grade": false,
     "grade_id": "cell-55bc3b6b8e363d8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reducing model capacity\n",
    "\n",
    "Another simple way to reduce overfitting is to reduce the capacity of the model. Let us use for the same regression task a much smaller network: an MLP with one hidden layer with five units, tanh nonlinearity in the hidden layer and a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed8be34b73c4292c4ccb6ab94891a8d4",
     "grade": false,
     "grade_id": "cell-c3212365fca02faa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MLPSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPSmall, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84bd14c9059bf1d35e5f7da5c89748dd",
     "grade": false,
     "grade_id": "cell-02263b4f925fab1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPSmall()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92ab8028cf7858b328770679910591de",
     "grade": false,
     "grade_id": "cell-1c6977bb9e6169e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    n_epochs = 10000\n",
    "    train_errors = []  # Keep track of the training data\n",
    "    val_errors = []  # Keep track of the validation data\n",
    "\n",
    "    x = x_train.to(device)\n",
    "    y = y_train.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 500 == 0:\n",
    "            train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "            val_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "            print_progress(epoch, train_errors[-1], val_errors[-1])\n",
    "\n",
    "    # Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "    tools.save_model(mlp, 'mlp_small.pth', confirm=False)\n",
    "else:\n",
    "    mlp = MLPSmall()\n",
    "    tools.load_model(mlp, 'mlp_small.pth', device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaa66f04123f6fdbe459b98134e2bfa7",
     "grade": false,
     "grade_id": "cell-f465cfe6bb4297e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "if not skip_training:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.loglog(train_errors)\n",
    "    ax.loglog(val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27ad95465d708b318b1d106f3dd2b3e8",
     "grade": false,
     "grade_id": "cell-443a252b39226e7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47b10e34b8b74687735e319758ddf6b4",
     "grade": true,
     "grade_id": "accuracy_small",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_small = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss by reducing model capacity: %.5f\" % test_loss_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a823400a6ae118ec9192c571132d78",
     "grade": false,
     "grade_id": "cell-26f532d3fb21f610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can summarize the results obtained with different regularization methods:\n",
    "print('No regularization: %.5f' % test_loss_no_regularization)\n",
    "print('Early stopping:    %.5f' % test_loss_early_stopping)\n",
    "print('Weight decay:      %.5f' % test_loss_weight_decay)\n",
    "print('Noise injection:   %.5f' % test_loss_inj_noise)\n",
    "print('Dropout:           %.5f' % test_loss_dropout)\n",
    "print('Small network:     %.5f' % test_loss_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ed55496cc341e9c0b2eb71158e81220",
     "grade": false,
     "grade_id": "cell-a9fc6f9740d33f87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The values of the hyperparameters (parameters of the training procedure) may have major impact on the results. One has to find the best hyperparameter values which is usually done by measuring the performance on the validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
