{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16c60efe7e59356e3d37718baac45947",
     "grade": false,
     "grade_id": "cell-87195ccb7e06731c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 8, 2023 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Train a multilayer perceptron (MLP) network in numpy.\n",
    "\n",
    "In this exercise, we implement training of a multilayer perceptron network using the `numpy` library.\n",
    "* We implement forward and backward computations required for computing the gradients with backpropagation.\n",
    "* We train an MLP on a toy data set.\n",
    "\n",
    "We will implement an MLP with two hidden layers like shown in this figure:\n",
    "\n",
    "<img src=\"mlp.png\" width=300 style=\"float: top;\">\n",
    "\n",
    "We will build the following computational graph:\n",
    "\n",
    "<img src=\"mlp_compgraph.png\" width=600 style=\"float: top;\">\n",
    "\n",
    "Note that the computational graph contains a mean-squared error (MSE) loss because we solve a regression problem.\n",
    "\n",
    "Recall what we discussed in the lecture:\n",
    "\n",
    "<img src=\"backprop_software.png\" width=800 style=\"float: top;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc7ed69dbca98d71bcd8f763e643f6a7",
     "grade": false,
     "grade_id": "cell-cafdead5e95c3773",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4958aaa6f0bc9d01c358d15e1c698a2",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During grading, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d86acc1148120d049457fdc7439961d",
     "grade": false,
     "grade_id": "cell-1b57af4f7c548374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Gradient of the loss\n",
    "\n",
    "We start by implementing the last block of the computational graph which is the mean-squared error loss:\n",
    "$$\n",
    "c = \\frac{1}{N} \\sum_{i=1}^N (y_i - t_i)^2\n",
    "$$\n",
    "where $y_i$ are the elements of an input vector $\\mathbf{y}$ and $t_i$ are the elements of the target vector $\\mathbf{t}$.\n",
    "\n",
    "In the code below, we define a class that performs forward and backward computations of this loss function. Your task is to implement the `backward` function which should compute the gradient $\\frac{\\partial c}{\\partial \\mathbf{y}}$.\n",
    "\n",
    "Note that we process all $N$ training examples at the same time. Therefore, our implementation operates with two-dimensional arrays of shape `(n_samples, some_size)` where `n_samples` is the number $N$ of training samples and `some_size` is the size of an intermediate vector produced inside the MLP (e.g., the number of neurons in a hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9559a39ad6bbb19c80e6001054fae18d",
     "grade": false,
     "grade_id": "MSELoss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y of shape (n_samples, ysize): Inputs of the loss function (can be, e.g., an output of a neural network).\n",
    "          target of shape (n_samples, ysize): Targets.\n",
    "        \n",
    "        Returns:\n",
    "          loss (float): The loss value.\n",
    "        \"\"\"\n",
    "        self.diff = diff = y - target  # Keep this for backward computations\n",
    "        c = np.sum(np.square(diff)) / diff.size\n",
    "        return c\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          dy of shape (n_samples, ysize): Gradient of the MSE loss wrt the inputs.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'diff'), \"Need to call forward() first\"\n",
    "        # YOUR CODE HERE\n",
    "        # dc/dy = 1/N \\sum^N_{i=1} (2yi - 2ti) = 1/N \\sum^N_{i=1} 2 * diff \n",
    "        dy = 2 * self.diff/ self.diff.size\n",
    "        return dy\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b193d7a9befef960baae9a6809382fba",
     "grade": false,
     "grade_id": "cell-ea0f3206867f3d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MSELoss_shapes():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "    dy = loss.backward()  # Do backward computations\n",
    "    assert dy.shape == y.shape, f\"Bad dy.shape: {dy.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50b49f6bef0d311048eb9019ffb4e770",
     "grade": false,
     "grade_id": "cell-c1c40dfe278fb3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can test our implementation by comparing the results of our backward computations with a [numerical estimate](https://en.wikipedia.org/wiki/Numerical_differentiation) of the gradient: Suppose we have function \n",
    "$f(\\mathbf{x})$ of a vector input $\\mathbf{x}$, then the gradient can be estimated numerically at a (randomly chosen) input $\\mathbf{x}$ by\n",
    "$$ \\nabla f(\\mathbf{x}) \\approx \\frac{f(\\mathbf{x} + \\epsilon) - f(\\mathbf{x} - \\epsilon)}{2\\epsilon}$$\n",
    "using small $\\epsilon$. Note that the numerical gradient is an approximation of the analytical one and therefore there will be a small numerical difference between them.\n",
    "\n",
    "The function that we import in the cell below implements numerical computations of the gradient of a given function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdf411c2a8dd9bb465f4c00c7352cc27",
     "grade": false,
     "grade_id": "cell-2561d9e518b3b4bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8824a44edc5561f43ce33c8e2a9c29f",
     "grade": false,
     "grade_id": "cell-ee82c925fae51626",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [-0.25130573 -0.77018452 -0.43724832]\n",
      "Numerical gradient:\n",
      " [-0.25130573 -0.77018452 -0.43724832]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We now compare our analytical computations of the gradient with its numerical estimate\n",
    "def test_MSELoss_backward():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "\n",
    "    dy = loss.backward()\n",
    "    print('Analytical gradient:\\n', dy)\n",
    "    dy_num = numerical_gradient(lambda y: loss.forward(y, target), y)\n",
    "    print('Numerical gradient:\\n', dy_num[0])\n",
    "    assert np.allclose(dy, dy_num), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a24dff52a0e9e23c15ebd791e6e6605",
     "grade": true,
     "grade_id": "cell-c8af99b85700fd63",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d33f0fd41d4e3918c600dfe8f788fad",
     "grade": false,
     "grade_id": "cell-b71f50dda717743d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Linear layer\n",
    "\n",
    "Next we implement a linear layer.\n",
    "\n",
    "The forward computations of the linear layer are\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In the backward pass, the linear layer receives the gradients wrt to the outputs $\\frac{\\partial c}{\\partial \\mathbf{y}}$ and it needs to compute:\n",
    "* the gradients wrt the layer parameters $\\mathbf{W}$ and $\\mathbf{b}$\n",
    "* the gradient $\\frac{\\partial c}{\\partial \\mathbf{x}}$ wrt the inputs.\n",
    "\n",
    "In the cell below, we define a class that resembles class [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear) of pytorch. It calls functions `linear_forward` and `linear_backward` that implement the forward and backward computations. We implmented it this way because it makes it easier to test `linear_forward` and `linear_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e346260841be24d0dfbdfd5de60ba4fb",
     "grade": false,
     "grade_id": "cell-422a5f3dad7faf7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (int): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize the weights\n",
    "        bound = 3 / np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-bound, bound, (out_features, in_features))\n",
    "        bound = 1 / np.sqrt(in_features)\n",
    "        self.b = np.random.uniform(-bound, bound, out_features)\n",
    "\n",
    "        self.grad_W = None  # Attribute to store the gradients wrt W\n",
    "        self.grad_b = None  # Attribute to store the gradients wrt b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_samples, xsize): Inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (n_samples, ysize): Outputs.\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy of shape (n_samples, ysize): Gradient of a loss wrt outputs.\n",
    "        \n",
    "        Returns:\n",
    "          dx of shape (n_samples, xsize): Gradient of a loss wrt inputs.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first\"\n",
    "        assert dy.ndim == 2 and dy.shape[1] == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52c10d9d190c43559fe2ed272ac34dbb",
     "grade": false,
     "grade_id": "cell-c7ec54dadaa14e7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your task is to implement `linear_forward` and `linear_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5a7030d9d61bd468bddbe83c6ae3c36",
     "grade": false,
     "grade_id": "cell-92fa2567f6455189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(x, W, b):\n",
    "    \"\"\"Forward computations in the linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x of shape (n_samples, xsize): Inputs.\n",
    "      W of shape (ysize, xsize): Weight matrix.\n",
    "      b of shape (ysize,): Bias term.\n",
    "\n",
    "    Returns:\n",
    "      y of shape (n_samples, ysize): Outputs.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    y = x @ W.T + b\n",
    "    return y\n",
    "\n",
    "    \n",
    "def linear_backward(dy, x, W, b):\n",
    "    \"\"\"Backward computations in the linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy of shape (n_samples, ysize): Gradient of a loss wrt outputs.\n",
    "      x of shape (n_samples, xsize): Inputs.\n",
    "      W of shape (ysize, xsize): Weight matrix.\n",
    "      b of shape (ysize,): Bias term.\n",
    "\n",
    "    Returns:\n",
    "      dx of shape (n_samples, xsize): Gradient of a loss wrt inputs.\n",
    "      dW of shape (ysize, xsize): Gradient wrt weight matrix W.\n",
    "      db of shape (ysize,): Gradient wrt bias term b.\n",
    "    \"\"\"\n",
    "    assert dy.ndim == 2 and dy.shape[1] == W.shape[0]\n",
    "    # y = Wx + b\n",
    "    dx = dy @ W # dy/dx = W\n",
    "    dW = dy.T @ x # dy/dW = x\n",
    "    db = np.sum(dy, axis = 0) # dy/db = 1 => dy = db\n",
    "    return dx, dW, db\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e75506b30da32b7fa723a95c25a7b7c",
     "grade": false,
     "grade_id": "cell-7d17ae70e7b767ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We test the shapes of the outputs\n",
    "def test_linear_shapes():\n",
    "    n_samples = 4\n",
    "    x = np.random.randn(n_samples, 2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    # Test shapes\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(n_samples * 3).reshape((n_samples, 3))\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    assert dW.shape == W.shape, f\"Bad dW.shape={dW.shape}, W.shape={W.shape}\"\n",
    "    assert db.shape == b.shape, f\"Bad db.shape={db.shape}, b.shape={b.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_linear_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0675e30663ed9f4668cac970c31694ca",
     "grade": false,
     "grade_id": "cell-1f307ba68557823b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can again test the backward computations by numerical differentiation.\n",
    "\n",
    "Note that function `numerical_gradient` imported above accepts functions `fun` that works only with *one-dimensional arrays* as inputs and outputs.\n",
    "\n",
    "Suppose we have function `fun(X)` which accepts a two-dimensional array `X` of shape `(n1, n2)` as input and produces a one-dimensional array `y` of shape `(ny,)` as output. We want to compute partial derivatives\n",
    "`d y[i] / d X[k,l]` for each output element `y[i]` and each element `X[k,l]` of the input matrix. We can to it in the following way.\n",
    "\n",
    "First, we define a function with one-dimensional inputs such that it can be passed to our `numerical_gradient`\n",
    "function. Function `fun2` reshapes a one-dimensional array passed to it and calls function `fun`:\n",
    "```\n",
    "fun2 = lambda A: fun(A.reshape(n1, n2))\n",
    "```\n",
    "\n",
    "Then we can call the `numerical_gradient` function:\n",
    "```\n",
    "A = np.random.randn(n1, n2)\n",
    "dA = numerical_gradient(fun2, A.flatten())\n",
    "```\n",
    "which will produce a two dimensional array of shape `(ny, n1*n2)` that will contain the required partial\n",
    "derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ab42cdea4b6b41dd2fe8cb46a1d873c",
     "grade": false,
     "grade_id": "cell-00b350d6c7ada8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [[10.43016997 -6.88280721]\n",
      " [13.48873639 -7.59882121]\n",
      " [16.54730281 -8.3148352 ]]\n",
      "Numerical gradient:\n",
      " [[10.43016997 -6.88280721]\n",
      " [13.48873639 -7.59882121]\n",
      " [16.54730281 -8.3148352 ]]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# We test the backward computations of d/dW by numerical differentiation\n",
    "def test_linear_numerically():\n",
    "    n_samples = 4\n",
    "    x = np.random.randn(n_samples, 2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(n_samples * 3).reshape((n_samples, 3))\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "\n",
    "    print('Analytical gradient:\\n', dW)\n",
    "    dW_num = numerical_gradient(lambda W: linear_forward(x, W.reshape(3, 2), b).flatten(), W.flatten())\n",
    "    dW_num = dW_num.reshape(y.shape + W.shape)\n",
    "    expected = (dy[:, :, None, None] * dW_num).sum(axis=(0,1))\n",
    "    print('Numerical gradient:\\n', expected)\n",
    "    assert np.allclose(dW, expected), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_linear_numerically()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4137e6ce36250cf8abafae53b40c87c7",
     "grade": false,
     "grade_id": "cell-91d19db375ecd0d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recommend you to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd9775725d04e74cef35241eb570f99",
     "grade": true,
     "grade_id": "linear_batch_Wb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a77743a622413602365f3d776e550013",
     "grade": true,
     "grade_id": "cell-6bad54818463363a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1316dccf770021830b55f35740560574",
     "grade": true,
     "grade_id": "linear_batch_x",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0e0dbbc7008220c348ff24e28fc9785",
     "grade": false,
     "grade_id": "cell-f6dc372ec175c898",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. The Tanh activation function\n",
    "\n",
    "Next we implement the Tanh activation function. The activation function is applied *element-wise* to input vector $\\mathbf{x}$ to produce outputs $\\mathbf{y}$:\n",
    "$$\n",
    "\\mathbf{y} = \\text{tanh}(\\mathbf{x}) \\quad \\text{such that} \\quad y_i = \\text{tanh}(x_i).\n",
    "$$\n",
    "\n",
    "When we backpropagate through that block, we need to transform the gradients $\\frac{\\partial c}{\\partial \\mathbf{y}}$ wrt to the outputs into the gradients wrt the inputs $\\frac{\\partial c}{\\partial \\mathbf{x}}$. Your task is to implement the forward and backward computations.\n",
    "\n",
    "Notes:\n",
    "* We recommend you to compare analytical and numerical computations of the gradient.\n",
    "* If you use function `numerical_gradient` to differentiate numerically `Tanh.forward()` using a one-dimensional array `x` as input, the output of `numerical_gradient` is a two-dimensional array (Jacobian matrix). We are interested only in the diagonal elements of that array because the nonlinearity is applied *element-wise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c45bf7f2f9ffc7216a2484471e5344c8",
     "grade": false,
     "grade_id": "cell-15a2ce86bee8cd17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_features,): Inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (n_features,): Outputs.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        self.x = x\n",
    "        y = np.tanh(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy of shape (n_features,): Gradient of a loss wrt outputs.\n",
    "        \n",
    "        Returns:\n",
    "          dx of shape (n_features,): Gradient of a loss wrt inputs.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        # To find dc/dx, we can use the chain rule of differentiation:\n",
    "\n",
    "        #dc/dx = dc/dy * dy/dx\n",
    "\n",
    "        #We are given dc/dy, which is the derivative of the loss function with respect to y. And we know that y = tanh(x), so dy/dx is given by:\n",
    "\n",
    "        #dy/dx = sech^2(x)\n",
    "\n",
    "        #Therefore, we can write: dc/dx = dc/dy * (1 - tanh^2(x))\n",
    "        \n",
    "        dx = dy * (1 - np.tanh(self.x) ** 2)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d03a6c844168fa1c36d7b63b4ad33a",
     "grade": false,
     "grade_id": "cell-a2624334df2e830f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Tanh_shapes():\n",
    "    x = np.random.randn(3)\n",
    "    act_fn = Tanh()\n",
    "    y = act_fn.forward(x)\n",
    "    dy = np.arange(1, 4)\n",
    "    dx = act_fn.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Tanh_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f3e52bc9f308d8c95157d35a24ba623",
     "grade": false,
     "grade_id": "cell-15e28241c83862b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4. Multilayer Perceptron (MLP)\n",
    "\n",
    "In the cell below, you need to implement an MLP with two hidden layers and `Tanh` nonlinearity. Use instances of classes `Linear` and `Tanh` in your implementation.\n",
    "\n",
    "Note:\n",
    "* For testing purposes, the instances of `Linear` and `Tanh` classes should be attributes of class `MLP` such as attribute `fc1` in the example below:\n",
    "```\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        self.fc1 = Linear(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8253881c939b881e706e023490792f0",
     "grade": false,
     "grade_id": "MLPBatch",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of inputs which should be equal to xsize.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        self.fc1 = Linear(in_features, hidden_size1)\n",
    "        self.tanh1 = Tanh()\n",
    "        self.fc2 = Linear(hidden_size1, hidden_size2)\n",
    "        self.tanh2 = Tanh()\n",
    "        self.fc3 = Linear(hidden_size2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_samples, xsize): Inputs.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (n_samples, ysize): Outputs.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        x = self.fc1.forward(x)         \n",
    "        x = self.tanh1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.tanh2.forward(x)\n",
    "        y = self.fc3.forward(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy of shape (n_samples, ysize): Gradient of a loss wrt outputs.\n",
    "        \n",
    "        Returns:\n",
    "          dx of shape (n_samples, xsize): Gradient of a loss wrt inputs.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        dy = self.fc3.backward(dy) \n",
    "        dy = self.tanh2.backward(dy)  \n",
    "        dy = self.fc2.backward(dy)  \n",
    "        dy = self.tanh1.backward(dy) \n",
    "        dx = self.fc1.backward(dy) \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2652272c9b14ad871a8b564d341ac032",
     "grade": false,
     "grade_id": "cell-07c80ef21983d673",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MLP_shapes():\n",
    "    n_samples = 10\n",
    "    x = np.random.randn(n_samples, 1)\n",
    "    mlp_batch = MLP(1, 10, 20, 1)\n",
    "    y = mlp_batch.forward(x)\n",
    "\n",
    "    dy = np.arange(n_samples).reshape((n_samples, 1))   # Dummy gradient of a loss function wrt MLP's outputs.\n",
    "    dx = mlp_batch.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MLP_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4813bc59781b9ebca195a13448f787de",
     "grade": true,
     "grade_id": "test_MLPBatch",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366fa51dd0dde9f63b4a0fd2a856224f",
     "grade": false,
     "grade_id": "cell-91d50afcfcbc324d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD7CAYAAAB37B+tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwI0lEQVR4nO3dd3xb5b348c+jZdnxikec4RAHMkgYGQ4JEAIJe13akNKklNuW0RRaeqHcLpqWlrb0Qsto6a+9bbiMUlrCXimBFogLoSXD2Yvs4Th7eEvWeH5/HMmW5SNb05Lj7/v10kvHZ+nrI/urR9/znOcorTVCCCF6L0u6AxBCCJEYSeRCCNHLSSIXQoheThK5EEL0cpLIhRCil5NELoQQvVzSErlSyqqUWqWUWpisfQohhOheMlvkdwGbkrg/IYQQUbAlYydKqXLgGuAB4J7u1i8pKdEVFRVxvVZTUxP9+vWLa9tUkrhiI3HFRuKKTabGBYnFVl1dfURrXdppgdY64QfwMlAJTAcWdrd+ZWWljtfixYvj3jaVJK7YSFyxkbhik6lxaZ1YbMAKbZJTlU7wEn2l1LXA1VrrryulpgPf1lpfa7LeXGAuQFlZWeWCBQvier3GxkZyc3PjDzhFJK7YSFyxkbhik6lxQWKxzZgxo1prPanTArPsHssD+B+gBtgFHACagee62kZa5D1H4oqNxBUbiSt2qWiRJ3yyU2t9r9a6XGtdAcwBPtBa35TofoUQQkQnKSc7hRAiWTweDzU1Nbhcrrj3UVBQwKZNmdmJLprYnE4n5eXl2O32qPaZ1ESuta4CqpK5TyFE31JTU0NeXh4VFRUopeLaR0NDA3l5eUmOLDm6i01rzdGjR6mpqWH48OFR7VOu7BRCZBSXy0VxcXHcSby3U0pRXFwc0zcSSeRCiIzTV5N4UKy/vyRyIbqydznsX5vuKITokiRyIbqy6Lvwwc/SHYUQXZJELkRXPM3gaUl3FEJ0SRK5EF3xusHXmu4oRA/70Y9+xG9+85u2n+fNm8fjjz+exoi6Jv3IheiK1208RFrc/9YGNtbWx7ydz+fDarWaLhs7OJ8f/8cZXW5/6623cv3113PXXXfh9/tZsGABy5YtizmOniKJXIiueF3g86Q7CtHDKioqKC4uZtWqVRw8eJAJEyZQXFyc7rAikkQuRFe8bvBJizxdums5R5KMC4Juu+02nnnmGQ4cOMAtt9yS0L5STWrkQnTF6wKv1Mj7opkzZ/LOO++wfPlyrrjiinSH0yVpkQsRic8L2ict8j7K4XAwY8YMCgsLI9bbM4UkciEiCSZw6bXSJ/n9fj755BNeeumldIfSLSmtCBFJsLeKlFb6nI0bNzJixAguueQSRo4cme5wuiUtciEi8QYGLZLSSp8zduxYduzYke4woiYtciEiCSZyvxf8/vTGIkQXJJELEUnohUBSJxcZTBK5EJF4Q8aDlvKKyGCSyIWIpEOLXK7uFJlLErkQkYQmchlvRWQwSeRCRNKhRS6JvC/7yU9+wsMPPxz39lVVVVx77bVJjKgjSeRCRNKhRi6lFZG5pB+5EJFIaSX9Fn0fDqyLebNsnxesEdLbwLPgqge73ccDDzzAs88+y9ChQyktLWX8+PFMnDiRlStXArB161bmzJlDdXW16fbvvPMOd999NyUlJUycOLFt/ne+8x0GDx7Mfffdx7vvvssDDzxAVVUVFkv87WpJ5EJE0qFFLt0P+5Lq6moWLFjAqlWr8Hq9TJw4kcrKSgoKCli9ejXjx4/n6aef5itf+Yrp9i6Xi69+9at88MEHjBgxgtmzZ7ctu//++7n44ouZNm0a//Vf/8Xbb7+dUBIHSeRCRCaJPP2iaDmbaUlwGNuPPvqImTNnkpOTA8B1110HGEPbPv300zz66KO88MILEW82sXnzZoYPH952ef9NN93E/PnzAcjJyeGJJ57gwgsv5LHHHuO0006LO84gqZELEUlo8pbSSp+jlOo0b9asWSxatIiFCxdSWVnZ5c0mzLYPWrduHcXFxdTW1iYlVknkQkQiLfI+68ILL+S1116jpaWFhoYG3nrrLQCcTidXXHEFd9xxBzfffHPE7U8//XR27tzJ9u3bAXj++efblu3Zs4dHHnmEVatWsWjRIpYuXZpwvJLIhYhELtHvsyZOnMjs2bMZP348s2bNYtq0aW3LvvjFL6KU4vLLL4+4vdPpZP78+VxzzTVccMEFDBs2DACtNXfeeScPP/wwgwcP5sknn+S2227D5XJF3Fc0pEYuRCShLXIprfQ58+bNY968eZ3mL1myhFtuuaXbm01ceeWVbN68udP8N998s61+X1lZybp1sffKCSeJXIhIpEUuwsycOZPt27fzwQcfpDuUDiSRCxGJtMhFmNdee63TvJkzZ7Jz584O8x566KEevc+nJHIhIvG2gr0feJrkys4eprXustdHJjFL7onSWse0vpzsFCISrwuyAn2RZayVHuN0Ojl69GjMyexkobXm6NGjOJ3OqLeRFrkQkXjd4MyHxgNSWulB5eXl1NTUcPjw4bj34XK5YkqEPSma2JxOJ+Xl5VHvUxK5EJF4XeDINaaltNJj7HY7w4cPT2gfVVVVTJgwIUkRJVcqYku4tKKUciqlliml1iilNiil7k9GYEKkndcN9myw2KW0IjJaMmrkbuBirfU4YDxwpVLq3CTsV4j08rrAlmU8vNL9UGSuhEsr2jgj0Rj40R549M2zFOLk4nODzQlWh/QjFxlNJePMsFLKClQDI4Dfaa2/Z7LOXGAuQFlZWeWCBQvieq3GxkZyc3MTiDY1JK7Y9Ia4zln2DZr6DaOgbiNHiyexZfSdGRFXJpG4YpdIbDNmzKjWWk/qtEBrnbQHUAgsBs7sar3Kykodr8WLF8e9bSpJXLHpFXE9dqbWr8xtf06jXnG8MkimxqV1YrEBK7RJTk1qP3Kt9QmgCrgymfsVIi28bqM+bs2S0orIaMnotVKqlCoMTGcDlwKdR4oRorfxuqRGLnqFZPQjHwT8KVAntwAvaq0XJmG/QqRXsEVuc8gFQSKjJaPXylogM3veCxEvrQOJ3CmlFZHxZKwVIcz4PIAO1MjtkshFRpNELoSZ4BC2NmfggiAprYjMJYlcCDPBxC29VkQvIIlcCDNtLXIprYjMJ4lcCDNtLXIprYjMJ4lcCDMdWuTSj1xkNknkQpjxhbXIJZGLDCaJXAgzHU52OmQYW5HRJJELYSZYWrEGSytSIxeZSxK5EGZCW+TB0kofvRmwyHySyIUwE3pBkNVuTMt9O0WGkkQuhJnwC4JAyisiY0kiF8JMeD9ykBa5yFiSyIUwE5rIg6UVuShIZChJ5EKYaauRO6S0IjKeJHIhzEhpRfQiksiFMON1gbKAxSalFZHxJJELYSZ4v06lpLQiMp4kciHM+FrbSyo2h/Esl+mLDCWJXAgzwRY5GJfogwycJTKWJHIhzHjd7Qm8rbQiiVxkJknkQpgJbZG3lVakRi4ykyRyIcx43e01cimtiAwniVwIM1IjF72IJHIhzHhDe60EnqW0IjKUJHIhzHRokcvJTpHZJJELYaZDjTw4HrkkcpGZJJELYcbrktKK6DUkkQthxus2Odkpg2aJzCSJXAgzoS1yixWUVcZaERlLErkQZnyt7S1yMJK6lFZEhpJELoSZ0BY5GOUVKa2IDCWJXIhwfn/nFrnVIaUVkbESTuRKqaFKqcVKqU1KqQ1KqbuSEZgQaRNM2MGTnBAorUj3Q5GZbEnYhxf4b631SqVUHlCtlPqH1npjEvYtRM9ru1+ntMhF75Bwi1xrvV9rvTIw3QBsAoYkul8h0qbtfp3hNXJpkYvMlNQauVKqApgALE3mfoXoUaE3Xg6yOaS0IjKW0lonZ0dK5QL/BB7QWr9qsnwuMBegrKyscsGCBXG9TmNjI7m5uYmEmhISV2wyOa4B6gSTl3+DjWP+m0NlFwIwYeV38VmdrB3307TFlanHS+KKTSKxzZgxo1prPanTAq11wg/ADrwL3BPN+pWVlTpeixcvjnvbVJK4YpPRcdWu0frH+VpvfKt9wVNXa/3UVemNKwNJXLFLJDZghTbJqcnotaKAJ4FNWutHE92fEGlnViO3OeSCIJGxklEjnwr8J3CxUmp14HF1EvYrRHq09VoJPdmZJb1WRMZKuPuh1noJoJIQixCZwexkp9UuV3aKjCVXdgoRzqxFLmOtiAwmiVyIcD6zFnmW9CMXGUsSuRDhTC8IsksiFxlLErkQ4YKlFWt4aUUSuchMksiFCBfxEn2pkYvMJIlciHBmg2bZpEYuMpckciHCRWqRaz/4vOmJSYguSCIXIpzXDRa7ca/OoLYbMEt5RWQeSeRChPO6O5ZVoL11LuUVkYEkkQsRzusyxlYJZbUHlkkiF5lHErkQ4cxa5MGuiFJaERlIErkQ4byujic6of1naZGLDCSJXIhwXpdJizxQWpEauchAksiFCOdr7dwil9KKyGCSyIUIZ9YiD578lNKKyECSyIUI53WbtMiD/cglkYvMI4lciHBeV8cBs0BKKyKjSSIXIpxZi1xKKyKDSSIXIpxZ90MprYgMJolciHAeF9izO86zyiX6InNJIhcinKcF7Dkd57WVVqRGLjKPJHIhwnlbTFrkUloRmUsSuRAhlN8XuCBIErnoPSSRCxHC4g+UTsJb5G1jrUhpRWQeSeRChLD4Ay3uiKUVT88GJEQUJJELEcLqi9Ait9gAJRcEiYwkiVyIEBFLK0oZ5RUprYgMZEt3AEJkkvbSSk6nZS5t42/Ld7DVt5lrzx7EGYPzUUr1cIRCdCYtciFCtJVWwkc/BFx+K16Pmyc+2sG1v13CV59d0cPRCWFOErkQIdpLKx1b5FprWvxWRhc7WD7vUm6eWsF7mw6xfl9dGqIUoiNJ5EKEsPrMe60canDj1nbyHZqifg7uvnQUTruFvy7bk4YohehIErkQISKd7Nx9tBkPNvJsfgAKsu1ce/Zg3li1j0a3t6fDFKIDSeRChIjU/XDX0SZasdHP6mubd+OUU2hq9fHm6tqeDFGITiSRCxEiUq+VPUebacWO09KeyCcMLeT0gXn8ddnungxRiE6SksiVUk8ppQ4ppdYnY39CpEtbaSWs18ruY81YbA4s/vYrO5VS3DjlFNbvq2ddjZz0FOmTrBb5M8CVSdqXEGkTqfvhnqNNWO2dLwj67IQhZNut0ioXaZWURK61/hA4lox9CZFOFn9g5ENLx3+NXUebsWblGHcPCpHvtHPduMG8vqqWI41y1adID6mRCxHC6nODvWNrvK7ZQ12LB4czx7jpRJi5F52K2+vjj//c3lNhCtFBj12ir5SaC8wFKCsro6qqKq79NDY2xr1tKklcscnUuE51N+HyW/gkJLaddcYJzha3F5freIdlQecNsvGnj3dyhu0AhVnJbx9l6vGSuGKXkti01kl5ABXA+mjWrays1PFavHhx3NumksQVm0yN68DvrtH68Ykd5r2xep8e9r2F+tiLd2r9YIXpdjsPN+pT7/2bvv/NDSmJK1OPl8QVu0RiA1Zok5wqpRUhQlh97k53B9pztAmA3Nx809IKQEVJP2ZNHMJzS3dzsN5luo4QqZKs7ofPA/8GRiulapRStyZjv0L0NIvfbXpV54C8LOzOfsb9PI1voJ188+KR+P2a3y/e1hOhCtEmWb1WvqC1HqS1tmuty7XWTyZjv0L0NKuv1TSRDyvOaZ/vNW9xDy3K4YZJQ/nrsj1sO9SY6lCFaCOlFSFCmLbIjzVxSlG/9pJLhPIKwD2XjSLHYeMHr67D7zdvuQuRbJLIhQhhdD9sT+Quj4+D9W4qQlvknuaI25fmZTHv6jEs23WMF1fsTXW4QgCSyIXowGiRt4+zsueYkbRPKc5pn99FixzghknlTBlexC/e3sThBrlISKSeJHIhQhhXdrZfELTriNFjZVhxv6ha5GCMwfKL68/C5fFz/1sbUharEEGSyIUIEV5aCbbIO5ZWum6RA5xWmsudF49g4dr9LFq3PyWxChEkiVz0LWtfhOO7zJdpjTWstLL7aDP5ThuFOY6oSytBd0w/jXHlBdz72joO1EnfcpE6kshF33F0O7z6VZg/A3b/q/Py4MiGIWOt7DraZJRVIKYWOYDdauGx2eNxe/x8+6U10otFpIwkctF3HP60ffpP18Hq5zsuD9a+Q1rk+463MLQokMCjrJGHOrU0lx9dO5Yl247w9L92xRG0EN2TRC76jiNbjOe5i2HYefD67bDxzfblwZZ2IGFrramta2FwQXgij65FHvSFyUO5dEwZDy3azIZauQGFSD5J5KLvOLIFcsugfwXc9CpYHbBvRfvy4BWbgQt/TjR7cHn8DCoMJvLYauRBSikemnUWRf0cfP0vK6l3ebrfSIgYSCIXfcfhT6FklDFttYOzEFwhLeS20oqRuGvrjIQ9uMDZYX4spZWg4twsfnvjBGqOt/Ddl9YGRwwVIikkkYu+QWs4shVKR7fPcxaEJfJgacVoedeeMFrobS3yKC7R78o5FUV878rRvLPhAE9/vCuufQhhRhK56BsaD4K7rr1FDl0kcqMFvj+8RW6xGBcLeeNL5ABfnXYql40t4xdvb2LFLrk7okgOSeSibwj2WAlN5NmF0HKi/eewk521J1zYrYqS3Kz2dWzOuFvkYNTLH75hHEOLcrj9uWr2nYh/X0IESSIXfUOwx0qXpZWO3Q/317UwsMCJxaLa17HnxFUjD1WQbeeJL03C7fXz1T+toLnVm9D+hJBELvqGI1vAkQd5g9rnhSfytl4rgdLKCReDCjoOaYs9O6EWedCIAbk8/oUJbD5QLxcLiYRJIhd9w+FPoWQkqJDWtbMQXCfa7/gT1iLfd6KlvT4eZM9JSiIHmDF6APdeNYa31x3gl+9+2v0GQkRgS3cAQvSII1vh1Is6znMWgN9rJHBHvw41cp9fc7DexeBCsxZ5YqWVULdNG87uY0384Z/bKc3L4tYLhidt36LvkEQuTn6uemioNVrkoZwFgeV1gUQeKK3YsznS6Mbr1+1dD4Ps2e3rJYFSivuvO5Ojja38bOFGSnIdfGb8kKTtX/QNUloRJ78jW43nktEd52cXGs/BniueZvzKClY7tSfCuh4GJblFDmC1KB6bPZ4pw4v49ktrWLz5UFL3L05+ksjFyc+sxwp0bJEDeFrwW4yuhm0XA6XoZGc4p93KE1+exOiBeXztz9W8t/Fg0l9DnLwkkYuT35FPwWIzxlgJFZ7IvS34LQ4g5GKgwtSd7AyX77Tzl1vPZcygPO74SzXvrD+QktcRJx9J5OLkd3gLFJ1mjK8SylloPLtOGM+eFnzW9hZ5jsNKQXbYNikorYQqyLHz59umcOaQAu7860reWL0vZa8lTh6SyMXJ78gWKB3VeX5bIg+WVprbSiv761oYVOBEhXZXhJSVVkLlO+08e8tkKof1564Fq/nt+1tlkC3RJUnk4uTmbYVjOzqf6ARw5hvPbYncFdIib+nc9RCM0oq3pb3veYrkOe08e+tkrp8whEf+sYUn17fS6vWn9DVF79Wruh9W7z7ORzUejlTXYFFgUQoVeLZaFBZldOeyKNVhubEsMK0UFkv7Oh2Wtc1v319w2hq+TeBna2C5X2u01p1bcCK9jm4D7es4xkqQ1Q72fiG9Vtpr5LV1LkYPzOu8TXAoW6+rw02aUyHLZuWRz4/jlOIcfv3eVmbP/zePz5nA0KKc7jcWfUqvSuSvrarhufWtsH5NukMx9+7bWBTYLBYslvYPDZul/cPAqhRWq8JmsWANX2ZR2IPzrcYym9XS9my3GsvtNoXdasFhtWAPPBw2Y3mW3UqW1UKW3UKWzcKWw16yth/FabeQ7bCSbbeS7bCS47CRY7d2HEfkZFS7yngePN58eXZhh9KKz5pFq9fPkUZ35x4r0HEo2xQncjAaJndfOgrPkT08u6mRqx//iF/OOpurzhrU/caiz+hVifzbl49mfNZhpkw5F7/W+PwajXFLLp8fNMY8f2DarzHW0R2nfcFtNW378XeYNpb5/Ma6/uCzxpgOrOMPvK7P72f7jp0MHVbRtq7P3/7wa43Xb+zH6++4zOPzty33+trnu7y+wHKN1+fH69e0ev14/X68Pk2rzx/42Vi/S9WfRFyU47CSm2Uj12kjz2kn32kjP9tO/xw7/XMc9M9xUJzroCQ3i9K8LMryneQ7bb3nm8e+amOMleKR5sudBe0nO70u/JYCDta70BqGmJZWQm8uUZSKiE2dM9DGnMvP587nV3HHX1ZyQ2U5P7h6DP37OXosBpG5elUiL8xxUJJtycivllWWfUyfbvL1vQf4Akm+1evH7fMZz14/bo+ffy1bztizxuH2+HF5fDS3+mj2+Ghp9dLo9tHk9tLk9tLg9tLg8lLf4qHmeAsnmlupa/Fg9hmR47AysMBJef8cTinK5pSiHE4tyWVkWS7l/XOwZlIrv3al0Rq3RDgdFDpwlqcZv720bWjZQeFdDyHu270lw9CiHF762nn8+r0t/PHDHby/+RA/vGYMMycM6T0frCIlelUiF+asFmWUTRxWoGN3uUOFVs4/rSSu/fr9mnqXhyONrRxpdHO4wc3Behf761zsr2th77EW1uw9QV1L+z0os2wWTh+Yx1nlBZw1pIDKYf05rTQ3PYnG64YD6+G8r0dex1kI9TXGtKcFnzOrrQ+5aWklgdu9JYPDZuG7V57Of4wbzA9eW8c9L67hr0v38J0rRjPl1OK0xCTSTxK5iMhiURTmOCjMcTBiQG7E9eqaPWw73Mj2Q418erCBDbV1vLGqluc+2QNASa6DycOLmDaylEtOH8CAfJOWbiocWA9+DwypjLyOswAObTCmPS78lqy2qzo7XQwEIYk8vTeEGDMon1duP58Fy/fy6/e2MHv+J0wbWcK3LhvFxFP6pzU20fMkkYuEFeTYqRzWn8ph7QnE79fsOtrE8l3HWLrjGJ/sOMrb64wrFceVFzA6p5UxE12UpTKp1640ngdPjLyOswBaQk92Othf10Jhjp0ch8m/RxpLK+EsFsWNU05h5oQh/PmTXfxv1Xau//2/GDe0kFumVnDVmYNw2KSHcV8giVykhMWiOLU0l1NLc5l9zilordlysJH3Nh3k7xsP8uIWDy//z/tcOKqUOeecwmVjy5JfW9+3EvqVQkF55HWyC8Fdb/Q393vwW7LMbygRFLifZyYk8qBsh5W5F57GjVOG8erKGp75eBd3LVjN/f02cs1Zg/jshMFMPKW/1NFPYpLIRY9QSjF6YB6jB+bxjRkjWPC3D6ixDeGVlTXc/lw1w0v6cesFw/lcZTlOuzU5L7qv2iirdJXAnAWA5uM1G5kKLDtipbr5OJWRyhNtLfL01Mi7kptl40vnVXDTlGF8tO0IL63Yy4sr9vLnT3YzuMDJ9NMHMGP0AKaOKDb/tiF6raS8m0qpK4HfAFbg/7TWDyZjv+LkNbCfhTnTR/Oty0bxzvoDzP9wOz98fT2Pv7+Vb18+mlmV5Ym10N0NxqX5Z84yXez3a372t43Y1tQyD/jVy/9kahZ8WmdjcEk210+M0IrPkBp5VywWxUWjSrloVCmNbi/vrj/AuxsO8Pqqffx16R5sFsWZQwqYPLyIymH9OWtIgflwBKLXSDiRK6WswO+Ay4AaYLlS6k2t9cZE9y1OflaL4pqzB3H1WQP5ZMcxfvnuZr77ylqe+ngn864Zw7SRpfHtuHY1oCOe6Pzn1sM8/fEu7hlaDIfhV1cMgCq4YWwu931hWuT9ZlCNPBq5WTZmVZYzq7Ict9fHil3HWbLtCCt2HeOZj3cx/8MdABT1czB2UD4jBuQyYkAup5XmUlGSQ1me8+S/aOwkkIwW+WRgm9Z6B4BSagHwGSD5iXz/GkoO/ws2htwwl5A/sogtikjrqJB5sa7TcaL/sXWwXXdct8MzYfMsnadVcNoSMj/kYbGAsgamrca0xWZMW2zGw2oP2XfvoZTivNOKefWO81m0/gAPLtrMfz65jM9VlvOja8ZSkGPvfieh9lUbz4MnmC5+8qOdDMx3cscVk+A5GJnTCIDf2s3J1zR3P0xEls3K1BElTB1hdEd1eXxsqK1nY20d6/fVs3F/PS+u2Etzq69tG4fVQnn/bAYVOhlUkM2gAicD8oyLw0rznBxs8lPv8pCX1YsuEjsJJSORDwH2hvxcA0wJX0kpNReYC1BWVkZVVVXMLzRyyx84s3YRbIgv0FQaB7A23VG08ysbWtk4X1lo/diO32LHb3GEPGfhswafnfisTry2HHzWbLy2HLy2XLy2PDz2XDz2AlodBd0nuRg0NjZG/BvIAX40Cd7cZufVlTX8Y/0+vnKGgwkDov9zHbvhXfKcZSxdvq7Tsr0NfpZsa+GGUXbWbNrNJGD3+k8YBjS1+rv+29R+pgO7tm5il7eL9ZKsq+OVqKHA0BK4qgT8OovjLs3+Js3hZj+HWzSHml0cONzCxr3HOOE2rqbOopUCmshTzTy/ZB35qoX+9lYKrW4Kra3kWj3kWLz0Ux6yLV6yLD6ylA+HxY9d+XEoH3alsVo0dqUDo/fpwMNMe2NItzWqzKYtaKUY6vGyZ/szgfmWTsuDz+3LOj6H7rvjdsF1uo4FMFnP0GotT/p7mYxEbvYx3Ond0FrPB+YDTJo0SU+fPj32V5o4iuUfXsk555wT3GlXL9n1Om3zdRfr0PU6IftYtXIlEyZMCFk35Llt3bBlHab9xoOQ6dD52g9+nzEAVOi032/cQNjvCTz7wOfB4veAz8P+PTspH1hq9MrwuY2LZLwu476TnmbwHAdXM7Q2GnVlv4eI7DmQNxDyBkP+YOg/zLhZQ/8KY1CqfqVRfxOoqqqiu7+Byy6Gr+2r4zsvr+U3K+u5eeog7r1qTHRd6lbdCadNNX2N/35xDTmO/fxwzgwK3LVQDcOKsmAPOPoVMLm7v80lWVSUl1ERz99wnKI5XkmhNdTXwrHtcGIv1NUYF0w1HITGg+imQ9B8HOWNUFryBx5hf0YebcWL8fBhwYcFPwo/FnQw+SmFCnlYCA6Cp9t+tihtbKHAQmAafzCVogL/Nz6vB6uFkP8hX8f/xzTSZ/2Ys6d/Man7TEYir8H4UA8qB2qTsN/O8gfTlFsBZWekZPeJqNvhgmHnpTuMTrZVVVEebQLQ2kj07npjREDXCWg+Bs1HoOkINB02/skb9sPeT2D9K8Y/SFB2EZSeDoPONvpuD5lo3NAh0uXxUThzSAFvfGMqDy7azFMf72TlnhP87sYJlPfvYpiGxsNQtxemfK3TokP1Lt5cs48vThlmlGsshcaChv0AbeORd6kHxiTvEe5G2L8a9q+FA+vg4HpjtMjwslFuWdtDlZ0BOUWQ3R+chWzcsY+xE86FrFzjBtb2HLA5jWNkywJrFh6sNLp91LV4aHR7qXd5aHB5aXR5aXR72+Y1uY15DSHzG4NDSLR4cUcxjK/DaiHPacOGhwEF+eRn28jLspMXGEMoP8tGQbbFeHZaKXBayHca03lZimybBRWpMRWc7/cRsTHWqbEWOg9AU7cx+ekxGYl8OTBSKTUc2AfMAW5Mwn5FT1PK6Cdtd0LugO7X93mMFtuxHUYPkcOb4dAmWPksLP2DsU5OCQyfBhXTYMSlRis+Rg6bhfv+YyznVPTnuy+v5ZrHl/C7GydywcgIQw9sWWQ8Dz2306Jn/70br19z89SKwM7zAAUNxsVKwWFsu2TP6ZU1clqbYdcS2FEFe/4N+9e0fxDnlkHZmTBsKpSMMD6AC08x+uDbIn+4HWqqYuyo6V2+rB3ob7MmPMBXq9dvJHeXkfiD0w1uT2Ce8SHQ4PKwfc8+nLkOGlxeDjc0UtdifHiE1v9NY7UqCrLt5GfbKcy2U5jjoCDbTkG2ncKckHkh0/1z7OQ57VH3svJtqet+pRglnMi11l6l1J3AuxjdD5/SWmdgFVskndUORcONx4hL2uf7vMZ9MvdVw66PYddHsOE1Y9nAs+D0a8lpGhLzy1111iDGDMrna3+u5stPL2Pe1WO4eWpFx5NsWsPS+TBgLJRP6rB9XbOH55bu5vKxZQwr7mfMtFiMG0wEWuTBG0t0qTe1yJuPwcY3YPPfjPfB6zJazEMmwQXfgqFTYNA4yCtLd6TdctgsFNkcFEXxgVBVdZTp0yd3mu/x+akPJPV6l4e6Fg/1LV7qWgLTLg8nmj3UB34+1OBi66EGTjR5aHB7I76eUlCQbYwYWhAyemgw0Rf2c1AUmG5sTX55Jyn9yLXWbwNvJ2Nf4iRgtRnlr7IzYOKXjOR6dLvRUt78N6h6kMloqH0GJnzR6OsdvBFyNypK+vHK18/nnhdW89OFG9m4v56ff/bM9ouI9vwbDq6Da3/doV6vteaHb6yn0eXlmxeHDWnrLIQTu4FoSyupuwFzUnhbYfNCWLMAtr9vnDspOhUqb4ZRl8Mp57dfodrH2K0WinOzKM6N4n0O4/X5qXd5OR4YGbSu2cPx5lZONHs40dzK8WYPJ1qM6cONbrYcbOREcytNYd8C7qmM/bW7I5d3idRTyvi6XvJNOP+b0HCQba8/yIj6f8PCb8HffwQT/hPOvb3zne5N5GbZ+MNNlfz6vS08/sE2NtbW8783TTRa2Uv/aHwonP35Dtu8vnofb62p5duXj+LMIWEfGiEfIj5rNKWV1N6AOW51+6D6aaj+EzQdgvxyOPfrcNbnYODZva5LaqaxWS0U9YvuG0Eot9cXSPoejjW1cnR78ru3SSIXPS+vjJqhn2HERY8a46Es/QMsfwKW/RHGfhYu+h4MOL3LXVgsinsuH824oYXc8+Iarv3tEh6/uowZm96Cc+8wTrwF7D3WzH2vb+Cciv7cMX1E552FJPLoWuROo9dPpji6HZY8ZrTA/V4YdQWccxucdklCJ5pFcmTZrAzIt7aN+lm1N/kfqPIui/RRCsorYdYTcNdao7W+9e/w+3Ph5Vvh8JZud3HJmDIWfvMChpf0Y/2bj+HXfpaXXo/Wxt2Xqncf564Fxu3eHv38ePMTUtmFbZO96mTnsR3wym3w/ybB2heh8itw12q48QUYeZkk8T5EWuQiMxQMgct+CuffBf96HJY9ARtehbPnwPTvdVlyGVqUw0u3TcD/yD9Z4q3kSy8eYNj7VRyqd9Pi8WG1KB6bPT7ynaWCLXJbdnTlh3Sf7Gw8DB/+ElY8BRY7nHen8egFJyxFakgiF5mlXzFcdr/ROl/ymJHQ170EE24y6r2lJrfTazhI1ut3gOc45974Ax6qG8nCtfuZMXoAU4YXcc7wIkq6OrnlLDSeo72ZcrpOdnpbjfJT1UPGN4KJXzLKUPlyI+a+ThK5yEz9SuCKB+C8b8CHv4JVzxkn8k672Ejq/Ssgd6BxIcsb3zCuTL3mURyjLmE2MPucU6J/rbZEHuW9YNNwsrP/sdXwh+8Y/fVHXAZX/ML8Q030SZLIRWbLHwzXPgbTfwDVz8CKJ+HlWzquU3YmzHqy2xOkEQVLK9F2yevJ0kr9fnj3XsZteM3oQnjji8bJTCFCSCIXvUNuKVz0Hbjgbjiw1rgSs/GgsWzcjYn1i25L5DGUVrwtRv/4VHXp8/tg+f/B+z8DXys7K25k+E2/7vIqS9F3SSIXvYvV3vXNlOPRlsijLK3YAh8aXlf0yT8W+6qN/vX71xilpKsfZve6vQyXJC4ikEQuRLD7oS3a0krIzSWSmcibj8HiB2D5k8bYJ597Gs6YGWj17+12c9F3SSIXItYWeYebSxQl/vo+r3Eid/ED4KqDyXPh4h8aY8AIEQVJ5ELEUyOHxE94ag2fLoL3fwqHNxkjRF75IAw8M7H9ij5HErkQMfcjT/B2b1rDjsXwwc+NenjRqTD7OTj9WhkPRcRFErkQ9mzjCsmYE3mMLXK/H7a+Cx89AjXLjUGtrvstjPuCcRJXiDhJIhdCKeNK0uEXRndOMZ4W+Za/w3s/gUMbjBs2XP2wcWWm9EQRSSCJXAiAS39sPO+t6n7dtkQexQiIR7bBu/cag4EVj4CZ8+HM66UFLpJKErkQsWo72dlNi3zdy/Da7Ua3xst/DpO/BrbEbncmhBlJ5ELEKpoa+Z6l8PodMHSy0R9cRiYUKSSJXIhYddf98PhuWHCjcePi2c8Zd50XIoVk5HkhYtXVyU5XPTw/B/weY4ArSeKiB0iLXIhY2boorSx5FA5vhptehZKRnZcLkQLSIhciVhYLWLOMERBD+X2w5gUYeQWcNiM9sYk+SRK5EPEwG5N81xJoqIWzP5+emESfJYlciHiY3YB57QuQlQ+jr0pPTKLPkkQuRDzCW+StzbDxDRh7XWrGKBeiC5LIhYhH+A2YP33buG/o2XPSF5PosySRCxGP8Bswr33BGARr2NT0xST6LEnkQsTD7mxvkTcegm3vw9k3GD1ahOhh0o9ciHjYc6DpCGx7D6qfAe2Ds2enOyrRR0kiFyIe9mw4tBGem2VcIDR5LgwYk+6oRB8liVyIeFTeDLkDjbvcD58mPVVEWkkiFyIep15kPITIAHJmRgghermEErlS6gal1AallF8pNSlZQQkhhIheoi3y9cD1wIdJiEUIIUQcEqqRa603ASilkhONEEKImEmNXAghejmlte56BaXeAwaaLJqntX4jsE4V8G2t9You9jMXmAtQVlZWuWDBgrgCbmxsJDc3N65tU0niio3EFRuJKzaZGhckFtuMGTOqtdadz0dqrRN+AFXApGjXr6ys1PFavHhx3NumksQVG4krNhJXbDI1Lq0Tiw1YoU1yqpRWhBCil+u2tNLlxkrNBH4LlAIngNVa6yui2O4wsDvOly0BjsS5bSpJXLGRuGIjccUmU+OCxGIbprUuDZ+ZUCJPB6XUCm1WI0oziSs2EldsJK7YZGpckJrYpLQihBC9nCRyIYTo5XpjIp+f7gAikLhiI3HFRuKKTabGBSmIrdfVyIUQQnTUG1vkQgghQmRkIu9qVEWl1L1KqW1KqU+VUqZdHZVSRUqpfyiltgae+6cgxheUUqsDj11KqdUR1tullFoXWC/ila9JjOsnSql9IbFdHWG9KwPHcJtS6vs9ENevlFKblVJrlVKvKaUKI6zXI8eru99fGR4PLF+rlJqYqlhCXnOoUmqxUmpT4O//LpN1piul6kLe3/tSHVfgdbt8X9J0vEaHHIfVSql6pdTdYev0yPFSSj2llDqklFofMi+qPJSU/0Wzq4TS/QDGAKMJu2IUGAusAbKA4cB2wGqy/S+B7wemvw88lOJ4HwHui7BsF1DSg8fuJxjDJXS1jjVw7E4FHIFjOjbFcV0O2ALTD0V6T3rieEXz+wNXA4sABZwLLO2B924QMDEwnQdsMYlrOrCwp/6eon1f0nG8TN7TAxj9rHv8eAEXAhOB9SHzus1DyfpfzMgWudZ6k9b6U5NFnwEWaK3dWuudwDZgcoT1/hSY/hPw2ZQEitESAT4PPJ+q10iBycA2rfUOrXUrsADjmKWM1vrvWmtv4MdPgPJUvl43ovn9PwM8qw2fAIVKqUGpDEprvV9rvTIw3QBsAoak8jWTqMePV5hLgO1a63gvNEyI1vpD4FjY7GjyUFL+FzMykXdhCLA35OcazP/Qy7TW+8H45wAGpDCmacBBrfXWCMs18HelVHVg4LCecGfg6+1TEb7ORXscU+UWjNabmZ44XtH8/mk9RkqpCmACsNRk8XlKqTVKqUVKqTN6KKTu3pd0/03NIXJjKh3HC6LLQ0k5bmm7Z6eKYlRFs81M5qWs202UMX6BrlvjU7XWtUqpAcA/lFKbA5/eKYkL+F/gZxjH5WcYZZ9bwndhsm3CxzGa46WUmgd4gb9E2E3Sj5dZqCbzwn//Hv1b6/DCSuUCrwB3a63rwxavxCgfNAbOf7wOjOyBsLp7X9J5vBzAdcC9JovTdbyilZTjlrZErrW+NI7NaoChIT+XA7Um6x1USg3SWu8PfL07lIoYlVI2jDskVXaxj9rA8yGl1GsYX6USSkzRHjul1BPAQpNF0R7HpMallPoycC1wiQ4UCE32kfTjZSKa3z8lx6g7Sik7RhL/i9b61fDloYlda/22Uur3SqkSrXVKxxWJ4n1Jy/EKuApYqbU+GL4gXccrIJo8lJTj1ttKK28Cc5RSWUqp4RifrMsirPflwPSXgUgt/ERdCmzWWteYLVRK9VNK5QWnMU74rTdbN1nC6pIzI7zecmCkUmp4oDUzB+OYpTKuK4HvAddprZsjrNNTxyua3/9N4EuB3hjnAnXBr8mpEjjf8iSwSWv9aIR1BgbWQyk1GeN/+GiK44rmfenx4xUi4rfidByvENHkoeT8L6b6bG48D4wEVAO4gYPAuyHL5mGc5f0UuCpk/v8R6OECFAPvA1sDz0UpivMZ4PaweYOBtwPTp2KchV4DbMAoMaT62P0ZWAesDfxBDAqPK/Dz1Ri9Irb3UFzbMGqBqwOPP6TzeJn9/sDtwfcT4yvv7wLL1xHDePsJxHQBxtfqtSHH6eqwuO4MHJs1GCeNz++BuEzfl3Qfr8Dr5mAk5oKQeT1+vDA+SPYDnkDuujVSHkrF/6Jc2SmEEL1cbyutCCGECCOJXAghejlJ5EII0ctJIhdCiF5OErkQQvRyksiFEKKXk0QuhBC9nCRyIYTo5f4/9K45zdXOqGAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's create an MLP with random weights and compute the derivative wrt the one-dimensional input\n",
    "def test_MLP_derivative():\n",
    "    n_samples = 100\n",
    "    x = np.linspace(-10, 10, n_samples)\n",
    "    mlp_batch = MLP(1, 10, 20, 1)\n",
    "    y = mlp_batch.forward(x.reshape((n_samples, 1))).flatten()\n",
    "\n",
    "    dy_dx = mlp_batch.backward(np.ones((n_samples, 1))).flatten()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.plot(x, dy_dx)\n",
    "    ax.grid(True)\n",
    "    ax.legend(['y', 'dy_dx'])\n",
    "\n",
    "test_MLP_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb865aabe7b04a79776e7f6fed6750f",
     "grade": false,
     "grade_id": "cell-d5136f1291f0c36a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can visually inspect whether the computations of the derivative seem correct.\n",
    "\n",
    "More importantly, we can compute the gradient of a loss wrt the parameters of the MLP. The gradients can be used to update the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "651b3f554cfd6e87e29e2791e44c95e0",
     "grade": false,
     "grade_id": "cell-0630dc5ad992327d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Training MLP network with backpropagation\n",
    "\n",
    "Now let us use our code to train an MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee87bef8654ebd1cc3b570a6d2bb880f",
     "grade": false,
     "grade_id": "cell-bb746d106b37391b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8407f8a910>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate toy data\n",
    "def get_data():\n",
    "    np.random.seed(2)\n",
    "    x = np.random.randn(100, 1)\n",
    "    x = np.sort(x, axis=0)\n",
    "\n",
    "    targets = 2 * np.sin(x * 2 * np.pi / 3)\n",
    "    targets = targets + 0.2 * np.random.randn(*targets.shape)\n",
    "\n",
    "    return x, targets\n",
    "\n",
    "x, targets = get_data()\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, targets, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acbdd59f113750f669bfeec0453e3163",
     "grade": false,
     "grade_id": "cell-b5925a7912f3191c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And train an MLP network using gradient descent\n",
    "from IPython import display\n",
    "\n",
    "mlp = MLP(1, 10, 11, 1)  # Create MLP network\n",
    "loss = MSELoss()  # Create loss\n",
    "if not skip_training:  # The trained MLP is not tested\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x, targets, '.')\n",
    "    learning_rate = 0.05\n",
    "    n_epochs = 1 if skip_training else 200\n",
    "    for i in range(n_epochs):\n",
    "        # Forward computations\n",
    "        y = mlp.forward(x)\n",
    "        c = loss.forward(y, targets)\n",
    "\n",
    "        # Backward computations\n",
    "        dy = loss.backward()\n",
    "        dx = mlp.backward(dy)\n",
    "\n",
    "        # Gradient descent update\n",
    "        #learning_rate *= 0.99  # Learning rate annealing\n",
    "        for module in mlp.__dict__.values():\n",
    "            if hasattr(module, 'W'):\n",
    "                module.W = module.W - module.grad_W * learning_rate\n",
    "                module.b = module.b - module.grad_b * learning_rate\n",
    "\n",
    "        ax.clear()\n",
    "        ax.plot(x, targets, '.')\n",
    "        ax.plot(x, y, 'r-')\n",
    "        ax.grid(True)\n",
    "        ax.set_title('Iteration %d/%d' % (i+1, n_epochs))\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        plt.pause(0.005)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "778ff4525fa210606bebf85f115fb080",
     "grade": false,
     "grade_id": "cell-ca3ef83db74fc3f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you implement the MLP correctly, you will see that the learned function fits the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    try:\n",
    "        save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if save != 'yes':\n",
    "            print('Model not saved.')\n",
    "        else:\n",
    "            with open('1_mlp_numpy.pkl', 'wb') as file:\n",
    "                pickle.dump(mlp, file)\n",
    "            print('Model saved to 1_mlp_numpy.pkl.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a841cc5550cab630b7712d6a929983cc",
     "grade": true,
     "grade_id": "cell-0fc79088a3192cd3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    with open('1_mlp_numpy.pkl', 'rb') as file:\n",
    "        mlp = pickle.load(file)\n",
    "        print('File loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa12fb08bf4a55316cbe45014823532c",
     "grade": true,
     "grade_id": "cell-424d0ffbfe9f90f6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 0.07085682086585905\n"
     ]
    }
   ],
   "source": [
    "y = mlp.forward(x)\n",
    "c = loss.forward(y, targets)\n",
    "print(f'MSE loss: {c}')\n",
    "assert c < 0.1, 'Poorly trained MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b23190edec8868ef4939e0d98e928c33",
     "grade": true,
     "grade_id": "cell-7a6d3244c2914995",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "295b11424526bb9dc1ea17bf141803b7",
     "grade": false,
     "grade_id": "cell-841919d678edba70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "Now you have implemented backpropagation and trained an MLP network using gradient descent.\n",
    "\n",
    "PyTorch makes it easier to create neural networks with different architectures and optimize its parameters using (variants of) gradient descent:\n",
    "* It contains multiple building blocks with forward and backward computations implemented.\n",
    "* It implements optimization methods that work well for neural networks.\n",
    "* Computations can be performed either on GPU or CPU using the same code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
