{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3013127801bc9c7d2e60f4bfa548c97",
     "grade": false,
     "grade_id": "cell-440df6cfa709812f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 15, 2023 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 3. Convolutional networks. ResNet.\n",
    "\n",
    "In the third part you need to train a convolutional neural network with a ResNet architecture [(He et al, 2016)](https://arxiv.org/abs/1512.03385)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cc4d569dc32e40fe066146a07b7c7b7",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48d33ffe246f5459117f53cac15b370d",
     "grade": false,
     "grade_id": "cell-fe95dcf02c6b9c5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f2b11aa8f0d0377563333bd78493751",
     "grade": false,
     "grade_id": "cell-e5b565cc4aae8e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## FashionMNIST dataset\n",
    "\n",
    "Let us use the FashionMNIST dataset. It consists of 60,000 training images of 10 classes: 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9fb758b86d03e9884930cd772a48671",
     "grade": false,
     "grade_id": "cell-8b0fded08998282c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale images to [-1, 1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "           'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0a51d172db30aa4849af83df78bc4e4",
     "grade": false,
     "grade_id": "cell-e505c3b987b78603",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "We create a network with an architecure inspired by [ResNet](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "### ResNet block\n",
    "Our ResNet consists of blocks with two convolutional layers and a skip connection.\n",
    "\n",
    "In the most general case, our implementation should have:\n",
    "\n",
    "<img src=\"resnet_block_04.png\" width=220 style=\"float: right;\">\n",
    "\n",
    "* Two convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * no bias terms\n",
    "    * padding with one pixel on both sides\n",
    "    * 2d batch normalization after each convolutional layer.\n",
    "\n",
    "* **The first convolutional layer also (optionally) has:**\n",
    "    * different number of input channels and output channels\n",
    "    * change of the resolution with stride.\n",
    "\n",
    "* The skip connection:\n",
    "    * simply copies the input if the resolution and the number of channels do not change.\n",
    "    * if either the resolution or the number of channels change, the skip connection should have one convolutional layer with:\n",
    "        * 1x1 convolution **without bias**\n",
    "        * change of the resolution with stride (optional)\n",
    "        * different number of input channels and output channels (optional)\n",
    "    * if either the resolution or the number of channels change, the 1x1 convolutional layer is followed by 2d batch normalization.\n",
    "\n",
    "* The ReLU nonlinearity is applied after the first convolutional layer and at the end of the block.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Batch normalization is expected to be right after a convolutional layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc44ddc69f370f1b5f1d68c0dfb0732f",
     "grade": false,
     "grade_id": "cell-0db2cc6de47fa70d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"resnet_blocks_123.png\" width=650 style=\"float: top;\">\n",
    "\n",
    "The implementation should also handle specific cases such as:\n",
    "\n",
    "Left: The number of channels and the resolution do not change.\n",
    "There are no computations in the skip connection.\n",
    "\n",
    "Middle: The number of channels changes, the resolution does not change.\n",
    "\n",
    "Right: The number of channels does not change, the resolution changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c970ca322994ba8941f555de4baf9b5",
     "grade": false,
     "grade_id": "cell-7c95703d5b7fa14c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your task is to implement this block. You should use the implementations of layers in `nn.Conv2d`, `nn.BatchNorm2d` as the tests rely on those implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d38adfbfa37edfb193033cf63136c4e",
     "grade": false,
     "grade_id": "cell-5694742fd919140f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels (int):  Number of input channels.\n",
    "          out_channels (int): Number of output channels.\n",
    "          stride (int):       Controls the stride.\n",
    "        \"\"\"\n",
    "        super(Block, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # Convolutional layer\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        # dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, \n",
    "        #                    return_indices=False, ceil_mode=False)\n",
    "        \n",
    "        # ReLU function\n",
    "        # torch.nn.ReLU(inplace=False)\n",
    "        \n",
    "        # Fully connected (fc) layer\n",
    "        # torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "        \n",
    "        # torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, \n",
    "        #                      track_running_stats=True, device=None, dtype=None)\n",
    "        # num_features (int) â€“ C from the 4D shape input (N, C, H, W)\n",
    "        \n",
    "        # torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, \n",
    "        #                    count_include_pad=True, divisor_override=None)\n",
    "        \n",
    "        # Two convolutional layers\n",
    "        # 3x3 kernel\n",
    "        # no bias terms\n",
    "        # padding with one pixel\n",
    "        # 2d batch normalization\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # different input and output channels\n",
    "        # change of resolution with stride \n",
    "        \n",
    "        # The skip connection:\n",
    "\n",
    "        # if either the resolution or the number of channels change, the skip connection should have one convolutional layer with:\n",
    "        # 1x1 convolution **without bias**\n",
    "        # change of the resolution with stride (optional)\n",
    "        # different number of input channels and output channels (optional)\n",
    "        # if either the resolution or the number of channels change, the 1x1 convolutional layer is followed by 2d batch normalization.\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip_model = nn.Sequential(\n",
    "               nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "               nn.BatchNorm2d(num_features=out_channels)\n",
    "           )\n",
    "        else:\n",
    "            # simply copies the input if the resolution and the number of channels do not change.\n",
    "            # This case happens if neither the resolution or the number of channels change, \n",
    "\n",
    "            # torch.nn.Identity(*args, **kwargs)[SOURCE]\n",
    "            # A placeholder identity operator that is argument-insensitive.\n",
    "            # nn.Identity does is forwarding the input given to it (basically no-op)\n",
    "            self.skip_model = nn.Identity()\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # The ReLU nonlinearity is applied after the first convolutional layer \n",
    "        # and at the end of the block.\n",
    "        y = self.conv1(x)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        \n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        \n",
    "        # The skip connection in a residual block is implemented using an element-wise addition \n",
    "        # operation between the output of the last convolutional layer (y) and the original input \n",
    "        # to the block (x). This addition operation is performed only if the input and output \n",
    "        # of the block have the same spatial dimensions. If the dimensions are different, \n",
    "        # the input is first passed through a 1x1 convolutional layer and batch normalization \n",
    "        # to match the output dimensions before being added to the output. \n",
    "        \n",
    "        y = y + self.skip_model(x)\n",
    "        y = self.relu2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e979d57af1e56d9111d2114424848f4",
     "grade": false,
     "grade_id": "cell-786f7a32a4638bbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Block_shapes():\n",
    "\n",
    "    # The number of channels and resolution do not change\n",
    "    batch_size = 20\n",
    "    x = torch.zeros(batch_size, 16, 28, 28)\n",
    "    block = Block(in_channels=16, out_channels=16)\n",
    "    y = block(x)\n",
    "    assert y.shape == torch.Size([batch_size, 16, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "    # Increase the number of channels\n",
    "    block = Block(in_channels=16, out_channels=32)\n",
    "    y = block(x)\n",
    "    assert y.shape == torch.Size([batch_size, 32, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "    # Decrease the resolution\n",
    "    block = Block(in_channels=16, out_channels=16, stride=2)\n",
    "    y = block(x)\n",
    "    assert y.shape == torch.Size([batch_size, 16, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "    # Increase the number of channels and decrease the resolution\n",
    "    block = Block(in_channels=16, out_channels=32, stride=2)\n",
    "    y = block(x)\n",
    "    assert y.shape == torch.Size([batch_size, 32, 14, 14]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_Block_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "275be744ec85aa3fbc1c7615f6dcc119",
     "grade": true,
     "grade_id": "test_Block",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tests.test_Block(Block)\n",
    "tests.test_Block_relu(Block)\n",
    "tests.test_Block_batch_norm(Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c1c1c641143426800b594ba413f0bb0",
     "grade": false,
     "grade_id": "cell-0162fb4406cb8e15",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Group of blocks\n",
    "\n",
    "ResNet consists of several groups of blocks. The first block in a group may change the number of channels (often multiples the number by 2) and subsample (using strides).\n",
    "\n",
    "<img src=\"resnet_group.png\" width=500 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7604079b8ef9c38ceff3a174c6ebd9c",
     "grade": false,
     "grade_id": "cell-b1930fef0ab076c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We implement a group of blocks in this cell\n",
    "class GroupOfBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks, stride=1):\n",
    "        super(GroupOfBlocks, self).__init__()\n",
    "\n",
    "        first_block = Block(in_channels, out_channels, stride)\n",
    "        other_blocks = [Block(out_channels, out_channels) for _ in range(1, n_blocks)]\n",
    "        self.group = nn.Sequential(first_block, *other_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.group(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca7a619c8d092a7bfbd2bbf151869a6b",
     "grade": false,
     "grade_id": "cell-7481f327aee03c38",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupOfBlocks(\n",
      "  (group): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU()\n",
      "      (skip_model): Sequential(\n",
      "        (0): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU()\n",
      "      (skip_model): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU()\n",
      "      (skip_model): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's print a block\n",
    "group = GroupOfBlocks(in_channels=10, out_channels=20, n_blocks=3)\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cb1e5b08bc951b44b050aea3378bee7",
     "grade": false,
     "grade_id": "cell-71e97cde35d51918",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### ResNet\n",
    "\n",
    "Next we implement a ResNet with the following architecture. It contains three groups of blocks, each group having two basic blocks.\n",
    "\n",
    "<img src=\"resnet.png\" width=900 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f14e0a104c5c3959f0db37071737091",
     "grade": false,
     "grade_id": "cell-f0c465ad6f77bfc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The cell below contains the implementation of our ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c47adb28f3a9bc968847259244dba81",
     "grade": false,
     "grade_id": "cell-b44918fa89e59c40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_blocks, n_channels=64, num_classes=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_blocks (list):   A list with three elements which contains the number of blocks in \n",
    "                             each of the three groups of blocks in ResNet.\n",
    "                             For instance, n_blocks = [2, 4, 6] means that the first group has two blocks,\n",
    "                             the second group has four blocks and the third one has six blocks.\n",
    "          n_channels (int):  Number of channels in the first group of blocks.\n",
    "          num_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        assert len(n_blocks) == 3, \"The number of groups should be three.\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.group1 = GroupOfBlocks(n_channels, n_channels, n_blocks[0])\n",
    "        self.group2 = GroupOfBlocks(n_channels, 2*n_channels, n_blocks[1], stride=2)\n",
    "        self.group3 = GroupOfBlocks(2*n_channels, 4*n_channels, n_blocks[2], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1)\n",
    "        self.fc = nn.Linear(4*n_channels, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "          verbose: True if you want to print the shapes of the intermediate variables.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        if verbose: print('conv1:  ', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        if verbose: print('bn1:    ', x.shape)\n",
    "        x = self.relu(x)\n",
    "        if verbose: print('relu:   ', x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        if verbose: print('maxpool:', x.shape)\n",
    "\n",
    "        x = self.group1(x)\n",
    "        if verbose: print('group1: ', x.shape)\n",
    "        x = self.group2(x)\n",
    "        if verbose: print('group2: ', x.shape)\n",
    "        x = self.group3(x)\n",
    "        if verbose: print('group3: ', x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        if verbose: print('avgpool:', x.shape)\n",
    "\n",
    "        x = x.view(-1, self.fc.in_features)\n",
    "        if verbose: print('x.view: ', x.shape)\n",
    "        x = self.fc(x)\n",
    "        if verbose: print('out:    ', x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5665577ab3f7ec61d13a6e922f424e8",
     "grade": false,
     "grade_id": "cell-bae324fd2f70f08e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "conv1:   torch.Size([32, 10, 28, 28])\n",
      "bn1:     torch.Size([32, 10, 28, 28])\n",
      "relu:    torch.Size([32, 10, 28, 28])\n",
      "maxpool: torch.Size([32, 10, 14, 14])\n",
      "group1:  torch.Size([32, 10, 14, 14])\n",
      "group2:  torch.Size([32, 20, 7, 7])\n",
      "group3:  torch.Size([32, 40, 4, 4])\n",
      "avgpool: torch.Size([32, 40, 1, 1])\n",
      "x.view:  torch.Size([32, 40])\n",
      "out:     torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_ResNet_shapes():\n",
    "    # Create a network with 2 block in each of the three groups\n",
    "    n_blocks = [2, 2, 2]  # number of blocks in the three groups\n",
    "    net = ResNet(n_blocks, n_channels=10)\n",
    "    net.to(device)\n",
    "\n",
    "    # Feed a batch of images from the training data to test the network\n",
    "    with torch.no_grad():\n",
    "        images, labels = next(iter(trainloader))\n",
    "        images = images.to(device)\n",
    "        print('Shape of the input tensor:', images.shape)\n",
    "\n",
    "        y = net.forward(images, verbose=True)\n",
    "        print(y.shape)\n",
    "        assert y.shape == torch.Size([trainloader.batch_size, 10]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_ResNet_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8434bd0f4b1ca9f2f79f7a45a833a644",
     "grade": false,
     "grade_id": "cell-6c4d4ed166bc0b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee2cb885eb38f03cec2a78348673e054",
     "grade": false,
     "grade_id": "cell-ca720538c5742051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49abc4f9c03a8d7261bff5739b8cfcd0",
     "grade": false,
     "grade_id": "cell-88a8ad2f40029ba7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "In the cell below, implement the training loop. The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.01.\n",
    "* Cross-entropy loss. Note that we did not use softmax nonlinearity in the final layer of our network. Therefore, we need to use a loss function with log_softmax implemented, such as [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
    "* Number of epochs: 10\n",
    "\n",
    "We recommend you to use function `compute_accuracy()` defined above to track the accaracy during training. The test accuracy should be above 0.9.\n",
    "\n",
    "**Note: function `compute_accuracy()` sets the network into the evaluation mode which changes the way the batch statistics are computed in batch normalization. You need to set the network into the training mode (by calling `net.train()`) when you want to perform training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b18740656309f582785e2d8e75ebd44",
     "grade": false,
     "grade_id": "cell-9de9a4ddf89efc42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (group1): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (group2): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (group3): GroupOfBlocks(\n",
       "    (group): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (skip_model): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network\n",
    "n_blocks = [2, 2, 2]  # number of blocks in the three groups\n",
    "net = ResNet(n_blocks, n_channels=16)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae914389933e3986497ab27451391731",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # Zero out the gradients\n",
    "    net.zero_grad()\n",
    "    \n",
    "    # The loss function\n",
    "    crossEntropyLoss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # The optimizers\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    \n",
    "    # Number of training epochs \n",
    "    epochs = 10\n",
    "    \n",
    "    # Training over the dataset multiple times\n",
    "    for epoch in range(epochs):\n",
    "        # Before training the network, we call net.train() \n",
    "        net.train()\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            labels_pred = net(images)\n",
    "            loss = crossEntropyLoss(labels_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        accuracy = compute_accuracy(net,testloader)\n",
    "        print(f\"Accuracy at epoch {epoch}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(net, '3_resnet.pth', confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5977cae6bbed2cff9205c565ce9087c",
     "grade": false,
     "grade_id": "cell-cd608a7294f2ceb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 3_resnet.pth.\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    net = ResNet(n_blocks, n_channels=16)\n",
    "    tools.load_model(net, '3_resnet.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "962fa753dc68eba46cfcf22c862dbf7d",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 0.922\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the network on the test images: %.3f' % accuracy)\n",
    "n_blocks = sum(type(m) == Block for _, m in net.named_modules())\n",
    "assert n_blocks == 6, f\"Wrong number ({n_blocks}) of blocks used in the network.\"\n",
    "\n",
    "assert accuracy > 0.9, \"Poor accuracy ({:.3f})\".format(accuracy)\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
